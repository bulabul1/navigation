# è®­ç»ƒç»„ä»¶æ£€æŸ¥æŠ¥å‘Š

**æ£€æŸ¥æ—¶é—´**: 2025-10-06  
**ç›®æ ‡**: ç¡®ä¿æ‰€æœ‰ç»„ä»¶å‚æ•°æ­£ç¡®æ›´æ–°

---

## âœ… 1. ç¼–ç å™¨å‚æ•°æ›´æ–°æœºåˆ¶

### **ä¼˜åŒ–å™¨åˆ›å»º** (`agsac_model.py` Line 244-257)

```python
# æ”¶é›†ç¼–ç å™¨å‚æ•°
encoder_params = []
encoder_params.extend(self.dog_encoder.parameters())
encoder_params.extend(self.pointnet.parameters())
encoder_params.extend(self.corridor_encoder.parameters())
encoder_params.extend(self.pedestrian_encoder.parameters())
encoder_params.extend(self.fusion.parameters())
# æ³¨æ„ï¼štrajectory_predictorå·²å†»ç»“ï¼Œä¸åŒ…å« âœ…

# åˆ›å»ºç‹¬ç«‹ä¼˜åŒ–å™¨
actual_encoder_lr = encoder_lr if encoder_lr is not None else critic_lr
self.encoder_optimizer = optim.Adam(encoder_params, lr=actual_encoder_lr)
```

**çŠ¶æ€**: âœ… **æ­£ç¡®**
- ç¼–ç å™¨æœ‰ç‹¬ç«‹ä¼˜åŒ–å™¨
- å­¦ä¹ ç‡å¯é…ç½®ï¼ˆ`encoder_lr`ï¼‰
- Predictoræ­£ç¡®æ’é™¤ï¼ˆå·²å†»ç»“ï¼‰

---

### **æ¢¯åº¦åå‘ä¼ æ’­** (`sac_agent.py` Line 332-355)

```python
# 1. æ¸…ç©ºæ¢¯åº¦
self.critic_optimizer.zero_grad()
self.actor_optimizer.zero_grad()

# 2. ç»„åˆLoss
combined_loss = critic_loss + actor_loss

# 3. ç»Ÿä¸€backwardï¼ˆæ¢¯åº¦ä¼ æ’­åˆ°æ‰€æœ‰æ¨¡å—ï¼‰
combined_loss.backward()  # âœ… æ¢¯åº¦ä¼ åˆ°Encoder

# 4. åˆ†åˆ«è£å‰ª
critic_grad_norm = clip_grad_norm_(self.critic.parameters(), ...)
actor_grad_norm = clip_grad_norm_(self.actor.parameters(), ...)

# 5. æ›´æ–°Criticå’ŒActor
self.critic_optimizer.step()
self.actor_optimizer.step()
```

**çŠ¶æ€**: âœ… **æ­£ç¡®**
- `combined_loss.backward()` ä¼šä¼ æ’­æ¢¯åº¦åˆ°æ‰€æœ‰å‚ä¸forwardçš„æ¨¡å—
- ç¼–ç å™¨å‚ä¸äº†Criticå’ŒActorçš„forwardï¼Œä¼šæ”¶åˆ°æ¢¯åº¦

---

### **ç¼–ç å™¨å‚æ•°æ›´æ–°** (`trainer.py` Line 445-469)

```python
# 1. æ¸…ç©ºç¼–ç å™¨æ¢¯åº¦
if hasattr(self.model, 'encoder_optimizer'):
    self.model.encoder_optimizer.zero_grad()

# 2. SACæ›´æ–°ï¼ˆå†…éƒ¨backwardï¼Œç¼–ç å™¨æ”¶åˆ°æ¢¯åº¦ï¼‰
losses = self.model.sac_agent.update(segment_batch)

# 3. æ›´æ–°ç¼–ç å™¨å‚æ•°
if hasattr(self.model, 'encoder_optimizer'):
    # æ”¶é›†ç¼–ç å™¨å‚æ•°
    encoder_params = []
    encoder_params.extend(self.model.dog_encoder.parameters())
    encoder_params.extend(self.model.pointnet.parameters())
    encoder_params.extend(self.model.corridor_encoder.parameters())
    encoder_params.extend(self.model.pedestrian_encoder.parameters())
    encoder_params.extend(self.model.fusion.parameters())
    
    # æ¢¯åº¦è£å‰ª
    encoder_grad_norm = clip_grad_norm_(encoder_params, max_norm=1.0)
    
    # æ›´æ–°å‚æ•°
    self.model.encoder_optimizer.step()
```

**çŠ¶æ€**: âœ… **æ­£ç¡®**
- åœ¨SACæ›´æ–°å‰æ¸…ç©ºç¼–ç å™¨æ¢¯åº¦
- SACæ›´æ–°åï¼Œç¼–ç å™¨å‚æ•°å·²æœ‰æ¢¯åº¦
- è£å‰ªåæ‰§è¡Œ`step()`æ›´æ–°å‚æ•°

---

## âœ… 2. SAC Agentæ›´æ–°æœºåˆ¶

### **Actoræ›´æ–°** (`sac_agent.py` Line 347-355)

```python
# æ¢¯åº¦è£å‰ª
actor_grad_norm = torch.nn.utils.clip_grad_norm_(
    self.actor.parameters(), self.max_grad_norm
)

# æ›´æ–°å‚æ•°
self.actor_optimizer.step()
```

**çŠ¶æ€**: âœ… **æ­£ç¡®**
- Actorå‚æ•°æ­£ç¡®æ›´æ–°
- å­¦ä¹ ç‡: `actor_lr` (é…ç½®æ–‡ä»¶: 0.00005)

---

### **Criticæ›´æ–°** (`sac_agent.py` Line 347-354)

```python
# æ¢¯åº¦è£å‰ª
critic_grad_norm = torch.nn.utils.clip_grad_norm_(
    self.critic.parameters(), self.max_grad_norm
)

# æ›´æ–°å‚æ•°
self.critic_optimizer.step()
```

**çŠ¶æ€**: âœ… **æ­£ç¡®**
- Criticå‚æ•°æ­£ç¡®æ›´æ–°
- å­¦ä¹ ç‡: `critic_lr` (é…ç½®æ–‡ä»¶: 0.00005)

---

### **Alphaæ›´æ–°** (`sac_agent.py` Line 368-404)

```python
if self.auto_entropy and self.alpha_optimizer is not None:
    self.alpha_optimizer.zero_grad()
    
    # è®¡ç®—alpha loss
    alpha_loss = -(
        self.log_alpha * (avg_log_prob + self.target_entropy)
    )
    
    # åå‘ä¼ æ’­
    alpha_loss.backward()
    self.alpha_optimizer.step()
    
    # æ›´æ–°alphaå€¼
    self.alpha = self.log_alpha.exp()
```

**çŠ¶æ€**: âœ… **æ­£ç¡®**
- Alphaè‡ªåŠ¨è°ƒæ•´ç†µç³»æ•°
- å­¦ä¹ ç‡: `alpha_lr` (é»˜è®¤: 3e-4)

---

### **Targetç½‘ç»œè½¯æ›´æ–°** (`sac_agent.py` Line 410-423)

```python
def soft_update_target(self):
    """è½¯æ›´æ–°targetç½‘ç»œ"""
    for param, target_param in zip(
        self.critic.parameters(),
        self.critic_target.parameters()
    ):
        target_param.data.copy_(
            self.tau * param.data + (1 - self.tau) * target_param.data
        )
```

**çŠ¶æ€**: âœ… **æ­£ç¡®**
- Targetç½‘ç»œæŒ‰tau=0.005è½¯æ›´æ–°
- Targetç½‘ç»œä¸å‚ä¸æ¢¯åº¦è®¡ç®—ï¼ˆ`requires_grad=False`ï¼‰

---

## âœ… 3. å‚æ•°æ›´æ–°æµç¨‹æ€»ç»“

### **å®Œæ•´æ›´æ–°é¡ºåº** (æ¯ä¸ªè®­ç»ƒæ­¥)

```
1. Trainer.train_step():
   â”œâ”€ é‡‡æ ·batch
   â”œâ”€ Re-encodeè§‚æµ‹ â†’ ç¼–ç å™¨forwardï¼ˆä¸backwardï¼‰
   â””â”€ SACæ›´æ–° â†“

2. SAC.update():
   â”œâ”€ Critic forward/backward â†’ æ”¶é›†æ¢¯åº¦
   â”œâ”€ Actor forward/backward â†’ æ”¶é›†æ¢¯åº¦
   â”œâ”€ Combined backward â†’ æ¢¯åº¦ä¼ åˆ°ç¼–ç å™¨
   â”œâ”€ Critic.step() â†’ æ›´æ–°Criticå‚æ•°
   â”œâ”€ Actor.step() â†’ æ›´æ–°Actorå‚æ•°
   â”œâ”€ Alpha.step() â†’ æ›´æ–°Alpha
   â””â”€ soft_update_target() â†’ æ›´æ–°Targetç½‘ç»œ

3. Trainerç»§ç»­:
   â””â”€ Encoder.step() â†’ æ›´æ–°ç¼–ç å™¨å‚æ•° âœ…
```

---

## âœ… 4. å­¦ä¹ ç‡é…ç½®éªŒè¯

### **å½“å‰é…ç½®** (`resume_training_tuned.yaml`)

```yaml
training:
  actor_lr: 0.00005       # 5e-5
  critic_lr: 0.00005      # 5e-5
  encoder_lr: 0.000025    # 2.5e-5 (0.5x critic_lr)
  alpha: 0.2
  gamma: 0.99
  tau: 0.005
```

### **ä¼˜åŒ–å™¨åˆ›å»ºéªŒè¯**

```python
# resume_train.py Line 97-100
model = AGSACModel(
    actor_lr=config.training.actor_lr,        # 5e-5 âœ…
    critic_lr=config.training.critic_lr,      # 5e-5 âœ…
    encoder_lr=getattr(config.training, 'encoder_lr', None),  # 2.5e-5 âœ…
    ...
)
```

**çŠ¶æ€**: âœ… **æ­£ç¡®**
- æ‰€æœ‰å­¦ä¹ ç‡æ­£ç¡®ä¼ é€’
- Encoderæœ‰ç‹¬ç«‹ä¸”è¾ƒä½çš„å­¦ä¹ ç‡

---

## âœ… 5. ç¯å¢ƒé…ç½®éªŒè¯

### **å¥–åŠ±å‡½æ•°å‚æ•°** (`resume_training_tuned.yaml`)

```yaml
env:
  corridor_penalty_weight: 4.0      # é™å‹
  corridor_penalty_cap: 6.0         # é™å‹
  progress_reward_weight: 20.0      # ä¿æŒ
  step_penalty_weight: 0.01         # é™å‹
  enable_step_limit: true
```

### **ä»£ç ä¸­çš„ç¡¬ç¼–ç å¥–åŠ±** (`agsac_environment.py`)

```python
# Line 1188: Goalå¥–åŠ±
goal_reached_reward = 100.0  # âœ… å·²ä¿®æ”¹

# Line 1191: Collisionæƒ©ç½š
collision_penalty = -40.0    # âœ… å·²ä¿®æ”¹

# Line 1212: Directionæƒé‡
direction_reward = direction_normalized * 0.5  # âœ… å·²ä¿®æ”¹

# Line 1223: Curvatureæƒé‡
curvature_reward = normalized_curvature * 0.8  # âœ… å·²ä¿®æ”¹
```

**çŠ¶æ€**: âœ… **å…¨éƒ¨æ­£ç¡®**

---

## âœ… 6. è®­ç»ƒæ¢å¤æœºåˆ¶

### **CheckpointåŠ è½½** (`resume_train.py` Line 160-167)

```python
trainer.load_checkpoint(args.checkpoint)

current_episode = trainer.episode_count
print(f"å½“å‰çŠ¶æ€:")
print(f"  - Episode: {current_episode}")
print(f"  - Total steps: {trainer.total_steps}")
print(f"  - Best eval return: {trainer.best_eval_return:.2f}")
```

### **çŠ¶æ€æ¢å¤å†…å®¹** (`trainer.py` Line 635-686)

```python
def load_checkpoint(self, checkpoint_path):
    checkpoint = torch.load(checkpoint_path)
    
    # 1. æ¨¡å‹å‚æ•°
    self.model.load_state_dict(checkpoint['model_state'])
    
    # 2. ä¼˜åŒ–å™¨çŠ¶æ€
    self.model.sac_agent.actor_optimizer.load_state_dict(...)
    self.model.sac_agent.critic_optimizer.load_state_dict(...)
    if hasattr(self.model, 'encoder_optimizer'):
        self.model.encoder_optimizer.load_state_dict(...)  # âœ…
    
    # 3. è®­ç»ƒçŠ¶æ€
    self.episode_count = checkpoint['episode']
    self.total_steps = checkpoint['total_steps']
    self.best_eval_return = checkpoint['best_eval_return']
    
    # 4. ReplayBuffer
    if 'buffer_state' in checkpoint:
        self.buffer = checkpoint['buffer_state']
```

**çŠ¶æ€**: âœ… **æ­£ç¡®**
- æ‰€æœ‰ä¼˜åŒ–å™¨çŠ¶æ€éƒ½ä¼šæ¢å¤
- åŒ…æ‹¬ç¼–ç å™¨ä¼˜åŒ–å™¨
- è®­ç»ƒè¿›åº¦å®Œæ•´æ¢å¤

---

## âœ… 7. é…ç½®ä¼ é€’é“¾éªŒè¯

```
resume_training_tuned.yaml
    â†“ (AGSACConfig.from_yaml)
TrainingConfig
    â†“ (resume_train.py Line 97-100)
AGSACModel.__init__
    â”œâ”€ actor_lr â†’ SACAgent (Line 232)
    â”œâ”€ critic_lr â†’ SACAgent (Line 233)
    â””â”€ encoder_lr â†’ encoder_optimizer (Line 255)
```

**çŠ¶æ€**: âœ… **å®Œæ•´ä¼ é€’**

---

## ğŸ¯ æ£€æŸ¥ç»“è®º

### **æ‰€æœ‰ç»„ä»¶å‚æ•°æ›´æ–°æ­£å¸¸**

| ç»„ä»¶ | ä¼˜åŒ–å™¨ | å­¦ä¹ ç‡ | æ›´æ–°ä½ç½® | çŠ¶æ€ |
|------|--------|--------|----------|------|
| **DogEncoder** | `encoder_optimizer` | 2.5e-5 | `trainer.py:469` | âœ… |
| **PointNet** | `encoder_optimizer` | 2.5e-5 | `trainer.py:469` | âœ… |
| **CorridorEncoder** | `encoder_optimizer` | 2.5e-5 | `trainer.py:469` | âœ… |
| **PedestrianEncoder** | `encoder_optimizer` | 2.5e-5 | `trainer.py:469` | âœ… |
| **Fusion** | `encoder_optimizer` | 2.5e-5 | `trainer.py:469` | âœ… |
| **Actor** | `actor_optimizer` | 5e-5 | `sac_agent.py:355` | âœ… |
| **Critic** | `critic_optimizer` | 5e-5 | `sac_agent.py:354` | âœ… |
| **Alpha** | `alpha_optimizer` | 3e-4 | `sac_agent.py:401` | âœ… |
| **Critic_Target** | - (è½¯æ›´æ–°) | - | `sac_agent.py:410` | âœ… |
| **TrajectoryPredictor** | - (å†»ç»“) | - | - | âœ… |

---

## ğŸš€ å¯ä»¥å®‰å…¨å¼€å§‹è®­ç»ƒï¼

**ç¡®è®¤äº‹é¡¹**:
- âœ… æ‰€æœ‰æ¨¡å—éƒ½æœ‰å¯¹åº”çš„ä¼˜åŒ–å™¨
- âœ… æ¢¯åº¦ä¼ æ’­è·¯å¾„å®Œæ•´
- âœ… å‚æ•°æ›´æ–°é€»è¾‘æ­£ç¡®
- âœ… å­¦ä¹ ç‡é…ç½®åˆç†
- âœ… Checkpointæ¢å¤æœºåˆ¶å®Œæ•´
- âœ… å¥–åŠ±å‡½æ•°å·²æŒ‰è¦æ±‚è°ƒæ•´
- âœ… ç¯å¢ƒå‚æ•°å·²æ­£ç¡®é…ç½®

**æ— ä»»ä½•é˜»ç¢è®­ç»ƒçš„é—®é¢˜ï¼**

