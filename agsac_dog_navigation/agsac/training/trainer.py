"""
AGSAC Trainer
AGSACè®­ç»ƒå™¨ - å®Œæ•´çš„è®­ç»ƒå¾ªç¯

åŠŸèƒ½:
1. è®­ç»ƒå¾ªç¯ç®¡ç†
2. æ•°æ®æ”¶é›†ï¼ˆç¯å¢ƒäº¤äº’ï¼‰
3. æ¨¡å‹æ›´æ–°ï¼ˆSACè®­ç»ƒï¼‰
4. è¯„ä¼°ä¸ç›‘æ§
5. Checkpointing
6. æ—¥å¿—è®°å½•
"""

import os
import time
import json
import torch
import numpy as np
import random
from typing import Dict, Optional, List, Tuple
from pathlib import Path
from collections import deque

try:
    from torch.utils.tensorboard import SummaryWriter
    TENSORBOARD_AVAILABLE = True
except ImportError:
    TENSORBOARD_AVAILABLE = False
    print("[Warning] TensorBoard not available. Install with: pip install tensorboard")

from ..models import AGSACModel
from ..envs import AGSACEnvironment
from .replay_buffer import SequenceReplayBuffer


class AGSACTrainer:
    """
    AGSACè®­ç»ƒå™¨
    
    å®Œæ•´çš„è®­ç»ƒæµç¨‹:
    1. ç¯å¢ƒäº¤äº’æ”¶é›†episode
    2. Episodeå­˜å…¥ReplayBuffer
    3. é‡‡æ ·segment batchè®­ç»ƒSAC
    4. å®šæœŸè¯„ä¼°
    5. ä¿å­˜checkpoint
    """
    
    def __init__(
        self,
        model: AGSACModel,
        env: AGSACEnvironment,
        eval_env: Optional[AGSACEnvironment] = None,
        buffer_capacity: int = 10000,
        seq_len: int = 16,
        burn_in: int = 0,
        batch_size: int = 32,
        warmup_episodes: int = 10,
        updates_per_episode: int = 100,
        eval_interval: int = 10,
        eval_episodes: int = 5,
        save_interval: int = 50,
        log_interval: int = 1,
        max_episodes: int = 1000,
        device: str = 'cpu',
        save_dir: str = './outputs',
        experiment_name: str = 'agsac_experiment',
        use_tensorboard: bool = True,
        eval_seed: Optional[int] = None
    ):
        """
        Args:
            model: AGSACModelå®ä¾‹
            env: è®­ç»ƒç¯å¢ƒ
            eval_env: è¯„ä¼°ç¯å¢ƒï¼ˆå¯é€‰ï¼Œé»˜è®¤ç”¨è®­ç»ƒç¯å¢ƒï¼‰
            buffer_capacity: ReplayBufferå®¹é‡
            seq_len: åºåˆ—æ®µé•¿åº¦
            burn_in: Burn-iné•¿åº¦
            batch_size: æ‰¹æ¬¡å¤§å°
            warmup_episodes: é¢„çƒ­episodesï¼ˆå¼€å§‹è®­ç»ƒå‰ï¼‰
            updates_per_episode: æ¯episodeçš„æ›´æ–°æ¬¡æ•°
            eval_interval: è¯„ä¼°é—´éš”ï¼ˆepisodesï¼‰
            eval_episodes: æ¯æ¬¡è¯„ä¼°çš„episodesæ•°
            save_interval: ä¿å­˜é—´éš”ï¼ˆepisodesï¼‰
            log_interval: æ—¥å¿—é—´éš”ï¼ˆepisodesï¼‰
            max_episodes: æœ€å¤§è®­ç»ƒepisodes
            device: è®¾å¤‡
            save_dir: ä¿å­˜ç›®å½•
            experiment_name: å®éªŒåç§°
        """
        self.model = model
        self.env = env
        self.eval_env = eval_env if eval_env is not None else env
        
        # è®­ç»ƒå‚æ•°
        self.batch_size = batch_size
        self.warmup_episodes = warmup_episodes
        self.updates_per_episode = updates_per_episode
        self.eval_interval = eval_interval
        self.eval_episodes = eval_episodes
        self.save_interval = save_interval
        self.log_interval = log_interval
        self.max_episodes = max_episodes
        self.device = torch.device(device)
        
        # ReplayBuffer
        self.buffer = SequenceReplayBuffer(
            capacity=buffer_capacity,
            seq_len=seq_len,
            burn_in=burn_in,
            device=device
        )
        
        # ä¿å­˜è·¯å¾„
        self.save_dir = Path(save_dir) / experiment_name
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.episode_count = 0
        self.total_steps = 0
        self.total_updates = 0
        
        # è®­ç»ƒå†å²
        self.train_history = {
            'episode_returns': [],
            'episode_lengths': [],
            'eval_returns': [],
            'actor_losses': [],
            'critic_losses': [],
            'alpha_losses': [],
            'alpha_values': [],
            # æ–°å¢ï¼šEpisodeè¯¦ç»†ä¿¡æ¯
            'done_reasons': [],           # ç»ˆæ­¢åŸå› : 'collision', 'goal_reached', 'max_steps'
            'collision_types': [],        # ç¢°æ’ç±»å‹: 'pedestrian', 'corridor', 'boundary', 'none'
            'corridor_violations': [],    # Corridorè¿è§„æ¬¡æ•°
            'avg_progress_reward': [],    # å¹³å‡progresså¥–åŠ±
            'avg_corridor_penalty': []    # å¹³å‡corridoræƒ©ç½š
        }
        
        # æœ€ä½³æ¨¡å‹è¿½è¸ª
        self.best_eval_return = -float('inf')
        
        # å›ºå®šè¯„ä¼°éšæœºç§å­ï¼ˆå¯é€‰ï¼‰
        self.eval_seed = eval_seed
        
        # æ€§èƒ½åˆ†ææ ‡å¿—ï¼ˆç¡®ä¿åªprofilingä¸€æ¬¡ï¼‰
        self._profiled_collect = False
        self._profiled_train = False
        self._profiled_episode = False
        
        # TensorBoard
        self.use_tensorboard = use_tensorboard and TENSORBOARD_AVAILABLE
        self.writer = None
        if self.use_tensorboard:
            tensorboard_dir = self.save_dir / 'tensorboard'
            self.writer = SummaryWriter(log_dir=str(tensorboard_dir))
            print(f"[Trainer] TensorBoardæ—¥å¿—: {tensorboard_dir}")
        
        print(f"[Trainer] åˆå§‹åŒ–å®Œæˆ")
        print(f"  - å®éªŒåç§°: {experiment_name}")
        print(f"  - ä¿å­˜ç›®å½•: {self.save_dir}")
        print(f"  - Bufferå®¹é‡: {buffer_capacity}")
        print(f"  - åºåˆ—é•¿åº¦: {seq_len}")
        print(f"  - Batchå¤§å°: {batch_size}")
        print(f"  - TensorBoard: {'å¯ç”¨' if self.use_tensorboard else 'ç¦ç”¨'}")
    
    def collect_episode(self, deterministic: bool = False) -> Dict:
        """
        æ”¶é›†ä¸€ä¸ªå®Œæ•´episode
        
        Args:
            deterministic: æ˜¯å¦ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
        
        Returns:
            episode_data: {
                'fused_states': List[Tensor],
                'actions': List[Tensor],
                'rewards': List[float],
                'dones': List[bool],
                'hidden_states': List[Dict],
                'geo_scores': List[float],
                'episode_return': float,
                'episode_length': int,
                'start_pos': np.ndarray,
                'goal_pos': np.ndarray,
                'actual_path': List[np.ndarray],
                'done_reason': str
            }
        """
        # åŒæ­¥episode_countåˆ°ç¯å¢ƒï¼ˆç¡®ä¿è¯¾ç¨‹å­¦ä¹ æ­£ç¡®ï¼‰
        if hasattr(self.env, 'episode_count'):
            self.env.episode_count = self.episode_count
        
        # é‡ç½®ç¯å¢ƒ
        obs = self.env.reset()
        
        # è®°å½•èµ·ç‚¹å’Œç»ˆç‚¹
        start_pos = self.env.robot_position.copy()
        goal_pos = self.env.goal_pos.copy()
        actual_path = [start_pos.copy()]  # è®°å½•å®é™…èµ°è¿‡çš„è·¯å¾„
        
        # åˆå§‹åŒ–hidden states
        hidden_states = self.model.init_hidden_states(batch_size=1)
        
        # Episodeæ•°æ®
        observations = []  # æ–°å¢ï¼šåŸå§‹è§‚æµ‹ï¼ˆç”¨äºè®­ç»ƒæ—¶é‡ç¼–ç ï¼‰
        fused_states = []
        actions = []
        rewards = []
        dones = []
        hidden_states_list = []
        geo_scores = []
        reward_infos = []  # æ–°å¢ï¼šæ”¶é›†æ¯æ­¥çš„å¥–åŠ±è¯¦æƒ…
        
        episode_return = 0.0
        episode_length = 0
        done_reason = 'unknown'
        
        done = False
        
        # æ€§èƒ½åˆ†æï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡collectæ—¶ï¼Œä¸ç®¡æ˜¯å¦resumeï¼‰
        enable_profile = not self._profiled_collect
        if enable_profile:
            self._profiled_collect = True  # æ ‡è®°å·²profiling
            profile_data = {
                'add_batch': [],
                'model_forward': [],
                'data_transfer': [],
                'env_step': []
            }
        
        while not done:
            step_start = time.time() if enable_profile else None
            
            # æ·»åŠ batchç»´åº¦
            t0 = time.time() if enable_profile else None
            obs_batch = self._add_batch_dim(obs)
            if enable_profile:
                profile_data['add_batch'].append(time.time() - t0)
            
            # Forward passï¼ˆè·å–fused_stateï¼‰
            t0 = time.time() if enable_profile else None
            with torch.no_grad():
                model_output = self.model(
                    obs_batch,
                    hidden_states=hidden_states,
                    deterministic=deterministic,
                    return_attention=False
                )
            if enable_profile:
                profile_data['model_forward'].append(time.time() - t0)
            
            # æå–æ•°æ®
            action = model_output['action'].squeeze(0)  # ç§»é™¤batchç»´åº¦
            fused_state = model_output['fused_state'].squeeze(0)
            new_hidden_states = model_output['hidden_states']
            
            # æ‰§è¡ŒåŠ¨ä½œ
            t0 = time.time() if enable_profile else None
            action_np = action.cpu().numpy()
            if enable_profile:
                profile_data['data_transfer'].append(time.time() - t0)
            
            t0 = time.time() if enable_profile else None
            next_obs, reward, done, info = self.env.step(action_np)
            if enable_profile:
                profile_data['env_step'].append(time.time() - t0)
            
            # è®°å½•å½“å‰ä½ç½®åˆ°è·¯å¾„
            actual_path.append(self.env.robot_position.copy())
            
            # è®°å½•ç»“æŸåŸå› 
            if 'done_reason' in info:
                done_reason = info['done_reason']
            
            # è®°å½•æ•°æ®
            # æ–°å¢ï¼šä¿å­˜åŸå§‹è§‚æµ‹ï¼ˆæ·±æ‹·è´å¹¶ç§»åˆ°CPUï¼ŒèŠ‚çœGPUæ˜¾å­˜ï¼‰
            obs_cpu = self._to_device_observation(obs_batch, 'cpu', deep_copy=True)
            observations.append(obs_cpu)
            fused_states.append(fused_state)
            actions.append(action)
            rewards.append(reward)
            dones.append(done)
            hidden_states_list.append(self._copy_hidden_states(hidden_states))
            
            # æ”¶é›†å¥–åŠ±è¯¦æƒ…ï¼ˆæ–°å¢collision_typeï¼‰
            reward_infos.append({
                'progress_reward': info.get('progress_reward', 0.0),
                'direction_reward': info.get('direction_reward', 0.0),
                'curvature_reward': info.get('curvature_reward', 0.0),
                'corridor_penalty': info.get('corridor_penalty', 0.0),
                'corridor_violation_distance': info.get('corridor_violation_distance', 0.0),
                'in_corridor': info.get('in_corridor', True),
                'goal_reached_reward': info.get('goal_reached_reward', 0.0),
                'collision_penalty': info.get('collision_penalty', 0.0),
                'collision_type': info.get('collision_type', 'none'),  # æ–°å¢ï¼šç¢°æ’ç±»å‹
                'step_penalty': info.get('step_penalty', 0.0),
            })
            
            if 'geometric_reward' in info:
                geo_scores.append(info['geometric_reward'])
            else:
                geo_scores.append(0.0)
            
            # æ›´æ–°çŠ¶æ€
            episode_return += reward
            episode_length += 1
            obs = next_obs
            hidden_states = new_hidden_states
            
            # é˜²æ­¢æ— é™å¾ªç¯
            if episode_length >= self.env.max_episode_steps:
                break
        
        # è¾“å‡ºæ€§èƒ½åˆ†æï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡collectæ—¶ï¼‰
        if enable_profile and profile_data['model_forward']:
            print("\n" + "="*70)
            print(f"ğŸ” æ€§èƒ½åˆ†æ: ç¯å¢ƒäº¤äº’ (Episode {self.episode_count})")
            print("="*70)
            
            import numpy as np
            
            # åªåˆ†æå‰10æ­¥
            max_analyze = min(10, len(profile_data['model_forward']))
            
            print(f"\nå‰{max_analyze}æ­¥è¯¦ç»†è€—æ—¶:")
            print("-"*70)
            for i in range(max_analyze):
                batch_time = profile_data['add_batch'][i] * 1000
                model_time = profile_data['model_forward'][i] * 1000
                transfer_time = profile_data['data_transfer'][i] * 1000
                env_time = profile_data['env_step'][i] * 1000
                total_time = batch_time + model_time + transfer_time + env_time
                
                print(f"Step {i+1:2d}: Total={total_time:6.1f}ms "
                      f"(Batch={batch_time:4.1f}ms + Model={model_time:5.1f}ms + "
                      f"Transfer={transfer_time:4.1f}ms + Env={env_time:6.1f}ms)")
            
            # ç»Ÿè®¡å¹³å‡å€¼
            print("\n" + "-"*70)
            print(f"å¹³å‡å€¼ (åŸºäº{max_analyze}æ­¥):")
            print("-"*70)
            avg_batch = np.mean(profile_data['add_batch'][:max_analyze]) * 1000
            avg_model = np.mean(profile_data['model_forward'][:max_analyze]) * 1000
            avg_transfer = np.mean(profile_data['data_transfer'][:max_analyze]) * 1000
            avg_env = np.mean(profile_data['env_step'][:max_analyze]) * 1000
            avg_total = avg_batch + avg_model + avg_transfer + avg_env
            
            print(f"  æ·»åŠ Batch:   {avg_batch:6.2f}ms ({avg_batch/avg_total*100:4.1f}%)")
            print(f"  æ¨¡å‹æ¨ç†:    {avg_model:6.2f}ms ({avg_model/avg_total*100:4.1f}%)")
            print(f"  æ•°æ®ä¼ è¾“:    {avg_transfer:6.2f}ms ({avg_transfer/avg_total*100:4.1f}%)")
            print(f"  ç¯å¢ƒæ‰§è¡Œ:    {avg_env:6.2f}ms ({avg_env/avg_total*100:4.1f}%)")
            print(f"  æ€»è®¡:        {avg_total:6.2f}ms")
            
            # é¢„ä¼°å®Œæ•´episodeæ—¶é—´
            print("\n" + "-"*70)
            print("å®Œæ•´Episodeé¢„ä¼°:")
            print("-"*70)
            print(f"  æ¯æ­¥å¹³å‡: {avg_total:.2f}ms")
            print(f"  50æ­¥:  {avg_total * 50 / 1000:.1f}s")
            print(f"  100æ­¥: {avg_total * 100 / 1000:.1f}s")
            print(f"  200æ­¥: {avg_total * 200 / 1000:.1f}s")
            print("="*70 + "\n")
        
        # æ„å»ºepisodeæ•°æ®
        episode_data = {
            'observations': observations,  # æ–°å¢ï¼šåŸå§‹è§‚æµ‹ï¼ˆç”¨äºè®­ç»ƒï¼‰
            'fused_states': fused_states,  # ä¿ç•™ï¼šå‘åå…¼å®¹
            'actions': actions,
            'rewards': rewards,
            'dones': dones,
            'hidden_states': hidden_states_list,
            'geo_scores': geo_scores,
            'reward_infos': reward_infos,  # æ–°å¢ï¼šå¥–åŠ±è¯¦æƒ…
            'episode_return': episode_return,
            'episode_length': episode_length,
            'start_pos': start_pos,
            'goal_pos': goal_pos,
            'actual_path': actual_path,
            'done_reason': done_reason
        }
        
        return episode_data
    
    def train_step(self) -> Dict[str, float]:
        """
        æ‰§è¡Œä¸€æ¬¡è®­ç»ƒæ›´æ–°ï¼ˆå¤šä¸ªsegment batchï¼‰
        
        Returns:
            avg_losses: å¹³å‡æŸå¤±
        """
        if len(self.buffer) < self.warmup_episodes:
            return {}
        
        # é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿bufferçœŸçš„æœ‰æ•°æ®ï¼ˆé˜²æ­¢ç©ºbufferé‡‡æ ·ï¼‰
        if len(self.buffer.episodes) == 0:
            print("[Warning] Bufferä¸ºç©ºï¼Œè·³è¿‡è®­ç»ƒ")
            return {}
        
        actor_losses = []
        critic_losses = []
        alpha_losses = []
        alpha_values = []
        
        # æ€§èƒ½åˆ†æï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡è®­ç»ƒæ—¶ï¼Œä¸ç®¡æ˜¯å¦resumeï¼‰
        enable_profile = not self._profiled_train
        if enable_profile:
            self._profiled_train = True  # æ ‡è®°å·²profiling
            profile_data = {
                'sample': [],
                'update': [],
                'total': []
            }
            print("\n" + "="*70)
            print("æ€§èƒ½åˆ†æ: è®­ç»ƒæ›´æ–° (å‰5æ¬¡)")
            print("="*70)
        
        for i in range(self.updates_per_episode):
            update_start = time.time() if enable_profile else None
            
            # é‡‡æ ·segment batch
            t0 = time.time() if enable_profile else None
            segment_batch = self.buffer.sample(self.batch_size)
            sample_time = (time.time() - t0) if enable_profile else 0
            
            # é‡æ–°ç¼–ç observationsï¼ˆä½¿ç¼–ç å™¨å‚ä¸æ¢¯åº¦ä¼ æ’­ï¼‰
            t_encode = time.time() if enable_profile else None
            if segment_batch and len(segment_batch[0].get('observations', [])) > 0:
                # æ‰¹é‡é‡ç¼–ç observationså’Œnext_observations
                for segment in segment_batch:
                    obs_list = segment['observations']
                    next_obs_list = segment['next_observations']
                    
                    # é‡æ–°ç¼–ç ï¼ˆwith gradï¼‰
                    if obs_list and len(obs_list) > 0:
                        # ç§»åŠ¨åˆ°æ¨¡å‹è®¾å¤‡ï¼ˆä»CPUæ¬å›GPUï¼‰
                        obs_on_device = [self._to_device_observation(obs, str(self.model.device)) for obs in obs_list]
                        next_obs_on_device = [self._to_device_observation(obs, str(self.model.device)) for obs in next_obs_list]
                        
                        # æ‰¹é‡ç¼–ç ï¼ˆå‡å°‘Pythonå¾ªç¯ï¼‰
                        fused_states_batch = self.model.encode_batch(obs_on_device)  # (seq_len, 64)
                        next_fused_states_batch = self.model.encode_batch(next_obs_on_device)  # (seq_len, 64)
                        
                        # æ›´æ–°segmentçš„stateså’Œnext_states
                        segment['states'] = fused_states_batch
                        segment['next_states'] = next_fused_states_batch
            
            encode_time = (time.time() - t_encode) if enable_profile else 0
            
            # SACæ›´æ–°ï¼ˆåŒæ—¶æ›´æ–°ç¼–ç å™¨ï¼‰
            t0 = time.time() if enable_profile else None
            
            # æ¸…ç©ºç¼–ç å™¨æ¢¯åº¦
            if hasattr(self.model, 'encoder_optimizer'):
                self.model.encoder_optimizer.zero_grad()
            
            # SACæ›´æ–°ï¼ˆå†…éƒ¨ä¼šç»„åˆCritic+Actor lossï¼Œä¸€æ¬¡backwardï¼‰
            # ç¼–ç å™¨æ¢¯åº¦æ¥è‡ªcombined_loss = critic_loss + actor_loss
            # - Critic loss â†’ å­¦ä¹ è¯„ä¼°ä»·å€¼çš„ç‰¹å¾
            # - Actor loss â†’ å­¦ä¹ é€‰æ‹©åŠ¨ä½œçš„ç‰¹å¾
            losses = self.model.sac_agent.update(segment_batch)
            
            # æ›´æ–°ç¼–ç å™¨å‚æ•°ï¼ˆä½¿ç”¨combined lossçš„æ¢¯åº¦ï¼‰
            if hasattr(self.model, 'encoder_optimizer'):
                # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
                encoder_params = []
                encoder_params.extend(self.model.dog_encoder.parameters())
                encoder_params.extend(self.model.pointnet.parameters())
                encoder_params.extend(self.model.corridor_encoder.parameters())
                encoder_params.extend(self.model.pedestrian_encoder.parameters())
                encoder_params.extend(self.model.fusion.parameters())
                
                encoder_grad_norm = torch.nn.utils.clip_grad_norm_(
                    encoder_params, max_norm=1.0
                )
                
                self.model.encoder_optimizer.step()
            
            update_time = (time.time() - t0) if enable_profile else 0
            
            # è®°å½•ç¼–ç æ—¶é—´ï¼ˆå¦‚æœprofilingï¼‰
            if enable_profile and i < 5:
                if 'encode' not in profile_data:
                    profile_data['encode'] = []
                profile_data['encode'].append(encode_time)
            
            # è®°å½•
            actor_losses.append(losses['actor_loss'])
            critic_losses.append(losses['critic_loss'])
            if 'alpha_loss' in losses:
                alpha_losses.append(losses['alpha_loss'])
            alpha_values.append(losses['alpha'])
            
            self.total_updates += 1
            
            # è®°å½•æ€§èƒ½æ•°æ®ï¼ˆåªè®°å½•å‰5æ¬¡ï¼‰
            if enable_profile and i < 5:
                total_time = time.time() - update_start
                profile_data['sample'].append(sample_time)
                profile_data['update'].append(update_time)
                profile_data['total'].append(total_time)
        
        # è¾“å‡ºæ€§èƒ½åˆ†æ
        if enable_profile and profile_data['total']:
            print("\næ¯æ¬¡æ›´æ–°çš„æ—¶é—´åˆ†å¸ƒ:")
            print("-"*70)
            for i in range(len(profile_data['total'])):
                sample_ms = profile_data['sample'][i] * 1000
                encode_ms = profile_data['encode'][i] * 1000 if 'encode' in profile_data and i < len(profile_data['encode']) else 0
                update_ms = profile_data['update'][i] * 1000
                total_ms = profile_data['total'][i] * 1000
                if encode_ms > 0:
                    print(f"  æ›´æ–°{i+1}: Sample={sample_ms:6.2f}ms  Encode={encode_ms:7.2f}ms  Update={update_ms:8.2f}ms  Total={total_ms:8.2f}ms")
                else:
                    print(f"  æ›´æ–°{i+1}: Sample={sample_ms:7.2f}ms  Update={update_ms:8.2f}ms  Total={total_ms:8.2f}ms")
            
            # å¹³å‡å€¼
            avg_sample = np.mean(profile_data['sample']) * 1000
            avg_encode = np.mean(profile_data['encode']) * 1000 if 'encode' in profile_data else 0
            avg_update = np.mean(profile_data['update']) * 1000
            avg_total = np.mean(profile_data['total']) * 1000
            
            print("-"*70)
            print("å¹³å‡è€—æ—¶:")
            print(f"  Bufferé‡‡æ ·:  {avg_sample:7.2f}ms ({avg_sample/avg_total*100:4.1f}%)")
            if avg_encode > 0:
                print(f"  é‡æ–°ç¼–ç :    {avg_encode:7.2f}ms ({avg_encode/avg_total*100:4.1f}%)")
            print(f"  SACæ›´æ–°:     {avg_update:7.2f}ms ({avg_update/avg_total*100:4.1f}%)")
            print(f"  å•æ¬¡æ€»è®¡:    {avg_total:7.2f}ms")
            
            # é¢„ä¼°å®Œæ•´train_stepæ—¶é—´
            estimated_total = avg_total * self.updates_per_episode / 1000
            print(f"\nå®Œæ•´train_stepé¢„ä¼° ({self.updates_per_episode}æ¬¡æ›´æ–°):")
            print(f"  é¢„ä¼°æ€»æ—¶é—´: {estimated_total:.1f}s")
            print("="*70 + "\n")
        
        # è®¡ç®—å¹³å‡
        avg_losses = {
            'actor_loss': np.mean(actor_losses),
            'critic_loss': np.mean(critic_losses),
            'alpha': np.mean(alpha_values)
        }
        
        if alpha_losses:
            avg_losses['alpha_loss'] = np.mean(alpha_losses)
        
        return avg_losses
    
    def evaluate(self) -> Dict[str, float]:
        """
        è¯„ä¼°å½“å‰ç­–ç•¥
        
        Returns:
            eval_stats: è¯„ä¼°ç»Ÿè®¡
        """
        # ä¿å­˜å½“å‰RNGçŠ¶æ€ä¸cudnné…ç½®
        rng_python = random.getstate()
        rng_numpy = np.random.get_state()
        rng_torch = torch.get_rng_state()
        rng_cuda = torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None
        cudnn_benchmark = torch.backends.cudnn.benchmark if torch.backends.cudnn.is_available() else None
        cudnn_deterministic = torch.backends.cudnn.deterministic if torch.backends.cudnn.is_available() else None
        
        # è®¾å®šè¯„ä¼°ç§å­ä»¥ä¿è¯ä¸€è‡´æ€§
        if self.eval_seed is not None:
            random.seed(self.eval_seed)
            np.random.seed(self.eval_seed)
            torch.manual_seed(self.eval_seed)
            if torch.cuda.is_available():
                torch.cuda.manual_seed_all(self.eval_seed)
            if torch.backends.cudnn.is_available():
                torch.backends.cudnn.benchmark = False
                torch.backends.cudnn.deterministic = True
        
        self.model.eval()
        
        eval_returns = []
        eval_lengths = []
        
        for _ in range(self.eval_episodes):
            episode_data = self.collect_episode(deterministic=True)
            eval_returns.append(episode_data['episode_return'])
            eval_lengths.append(episode_data['episode_length'])
        
        self.model.train()
        
        # æ¢å¤RNGçŠ¶æ€ä¸cudnné…ç½®
        random.setstate(rng_python)
        np.random.set_state(rng_numpy)
        torch.set_rng_state(rng_torch)
        if torch.cuda.is_available() and rng_cuda is not None:
            torch.cuda.set_rng_state_all(rng_cuda)
        if torch.backends.cudnn.is_available():
            if cudnn_benchmark is not None:
                torch.backends.cudnn.benchmark = cudnn_benchmark
            if cudnn_deterministic is not None:
                torch.backends.cudnn.deterministic = cudnn_deterministic
        
        eval_stats = {
            'eval_return_mean': np.mean(eval_returns),
            'eval_return_std': np.std(eval_returns),
            'eval_length_mean': np.mean(eval_lengths)
        }
        
        return eval_stats
    
    def train(self) -> Dict:
        """
        å®Œæ•´è®­ç»ƒå¾ªç¯
        
        Returns:
            train_history: è®­ç»ƒå†å²
        """
        # ä¿å­˜èµ·å§‹episodeï¼ˆæ”¯æŒresumeï¼‰
        start_episode = self.episode_count
        
        print(f"\n{'='*60}")
        print(f"å¼€å§‹è®­ç»ƒ: {self.max_episodes} episodes")
        if start_episode > 0:
            print(f"ä» Episode {start_episode} ç»§ç»­")
        print(f"{'='*60}\n")
        
        start_time = time.time()
        
        for episode in range(self.max_episodes):
            self.episode_count = start_episode + episode + 1
            
            # æ€§èƒ½åˆ†ææ ‡è®°ï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡æœ‰è®­ç»ƒçš„episodeï¼‰
            will_train = len(self.buffer) >= self.warmup_episodes
            enable_episode_profile = (not self._profiled_episode and will_train)
            if enable_episode_profile:
                self._profiled_episode = True  # æ ‡è®°å·²profiling
            
            # 1. æ”¶é›†episode
            episode_start = time.time()
            collect_start = time.time()
            episode_data = self.collect_episode(deterministic=False)
            collect_time = time.time() - collect_start
            
            buffer_start = time.time()
            self.buffer.add_episode(episode_data)
            buffer_time = time.time() - buffer_start
            
            episode_return = episode_data['episode_return']
            episode_length = episode_data['episode_length']
            self.total_steps += episode_length
            
            # è®°å½•è®­ç»ƒå†å²
            self.train_history['episode_returns'].append(episode_return)
            self.train_history['episode_lengths'].append(episode_length)
            
            # 2. è®­ç»ƒï¼ˆå¦‚æœbufferè¶³å¤Ÿï¼‰
            train_losses = {}
            train_time = 0
            if len(self.buffer) >= self.warmup_episodes:
                train_start = time.time()
                train_losses = self.train_step()
                train_time = time.time() - train_start
                
                # è®°å½•losses
                if train_losses:
                    self.train_history['actor_losses'].append(train_losses['actor_loss'])
                    self.train_history['critic_losses'].append(train_losses['critic_loss'])
                    if 'alpha_loss' in train_losses:
                        self.train_history['alpha_losses'].append(train_losses['alpha_loss'])
                    self.train_history['alpha_values'].append(train_losses['alpha'])
            
            episode_time = time.time() - episode_start
            
            # è¾“å‡ºepisodeçº§åˆ«çš„æ€§èƒ½åˆ†æï¼ˆç¬¬ä¸€æ¬¡æœ‰è®­ç»ƒæ—¶ï¼‰
            if enable_episode_profile:
                other_time = episode_time - collect_time - buffer_time - train_time
                print("\n" + "="*70)
                print("æ€§èƒ½åˆ†æ: Episodeçº§åˆ«æ—¶é—´åˆ†å¸ƒ")
                print("="*70)
                print(f"Episodeé•¿åº¦: {episode_length}æ­¥")
                print(f"Bufferå¤§å°: {len(self.buffer)}ä¸ªepisodes")
                print("-"*70)
                print(f"  1. æ”¶é›†Episode:    {collect_time:8.2f}s ({collect_time/episode_time*100:5.1f}%)")
                print(f"  2. æ·»åŠ åˆ°Buffer:   {buffer_time:8.2f}s ({buffer_time/episode_time*100:5.1f}%)")
                print(f"  3. è®­ç»ƒæ›´æ–°:       {train_time:8.2f}s ({train_time/episode_time*100:5.1f}%)")
                print(f"  4. å…¶ä»–(æ—¥å¿—ç­‰):   {other_time:8.2f}s ({other_time/episode_time*100:5.1f}%)")
                print("-"*70)
                print(f"  æ€»è®¡:              {episode_time:8.2f}s")
                print("="*70 + "\n")
            
            # 3. æ—¥å¿—ï¼ˆæ¯ä¸ªepisodeéƒ½è¾“å‡ºï¼‰
            self._log_episode(episode, episode_return, episode_length, 
                             train_losses, episode_time, episode_data)
            
            # 4. è¯„ä¼°
            if episode % self.eval_interval == 0 and episode > 0:
                eval_stats = self.evaluate()
                self.train_history['eval_returns'].append(eval_stats['eval_return_mean'])
                
                self._log_evaluation(episode, eval_stats)
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if eval_stats['eval_return_mean'] > self.best_eval_return:
                    self.best_eval_return = eval_stats['eval_return_mean']
                    self.save_checkpoint(is_best=True)
                    print(f"  [Best] æ–°çš„æœ€ä½³æ¨¡å‹! Return={self.best_eval_return:.2f}")
            
            # 5. å®šæœŸä¿å­˜
            if episode % self.save_interval == 0 and episode > 0:
                self.save_checkpoint(is_best=False)
        
        total_time = time.time() - start_time
        
        # æœ€ç»ˆä¿å­˜
        self.save_checkpoint(is_best=False, suffix='final')
        self.save_training_history()
        
        # å…³é—­TensorBoard
        if self.use_tensorboard and self.writer is not None:
            self.writer.close()
            print("[TensorBoard] æ—¥å¿—å·²ä¿å­˜å¹¶å…³é—­")
        
        print(f"\n{'='*60}")
        print(f"è®­ç»ƒå®Œæˆ!")
        print(f"  æ€»æ—¶é—´: {total_time/60:.2f} åˆ†é’Ÿ")
        print(f"  æ€»episodes: {self.episode_count}")
        print(f"  æ€»steps: {self.total_steps}")
        print(f"  æ€»updates: {self.total_updates}")
        print(f"  æœ€ä½³eval return: {self.best_eval_return:.2f}")
        print(f"{'='*60}\n")
        
        return self.train_history
    
    def _add_batch_dim(self, obs: Dict) -> Dict:
        """
        ä¸ºè§‚æµ‹æ·»åŠ batchç»´åº¦å¹¶è½¬æ¢ä¸ºAGSACModelæœŸæœ›çš„æ ¼å¼
        
        ç¯å¢ƒæ ¼å¼ â†’ æ¨¡å‹æ ¼å¼:
        {
            'robot_state': {...},
            'pedestrian_observations': (10, 8, 2),
            'corridor_vertices': (10, 20, 2),
            ...
        }
        â†’
        {
            'dog': {
                'trajectory': (1, 8, 2),
                'velocity': (1, 2),
                'position': (1, 2),
                'goal': (1, 2)
            },
            'pedestrians': {
                'trajectories': (1, 10, 8, 2),
                'mask': (1, 10)
            },
            'corridors': {
                'polygons': (1, 10, 20, 2),
                'vertex_counts': (1, 10),
                'mask': (1, 10)
            }
        }
        """
        # æå–æœºå™¨ç‹—ä¿¡æ¯
        robot_state = obs['robot_state']
        position = robot_state['position'].unsqueeze(0)  # (1, 2)
        velocity = robot_state['velocity'].unsqueeze(0)  # (1, 2)
        goal = obs['goal'].unsqueeze(0)  # (1, 2)
        
        # æ„é€ trajectoryï¼ˆä½¿ç”¨çœŸå®path_historyè€Œä¸æ˜¯é‡å¤å½“å‰ä½ç½®ï¼‰
        obs_horizon = self.env.obs_horizon
        if hasattr(self.env, 'path_history') and len(self.env.path_history) > 0:
            # ä»path_historyå–æœ€è¿‘obs_horizonä¸ªç‚¹
            path_hist = self.env.path_history[-obs_horizon:]
            # ä¸è¶³æ—¶ç”¨èµ·ç‚¹å¡«å……
            while len(path_hist) < obs_horizon:
                if hasattr(self.env, 'start_pos'):
                    path_hist.insert(0, self.env.start_pos.copy())
                else:
                    path_hist.insert(0, path_hist[0].copy() if path_hist else position.squeeze(0).cpu().numpy())
            # ä¼˜åŒ–ï¼šå…ˆè½¬numpy arrayå†è½¬tensorï¼Œé¿å…æ€§èƒ½è­¦å‘Š
            path_hist_array = np.array(path_hist, dtype=np.float32)  # (obs_horizon, 2)
            trajectory = torch.from_numpy(path_hist_array).to(position.device).unsqueeze(0)  # (1, obs_horizon, 2)
        else:
            # å›é€€ï¼šä½¿ç”¨positioné‡å¤ï¼ˆå‘åå…¼å®¹ï¼‰
            trajectory = position.unsqueeze(1).repeat(1, obs_horizon, 1)  # (1, obs_horizon, 2)
        
        # æ„é€ æ¨¡å‹æœŸæœ›çš„æ ¼å¼
        model_obs = {
            'dog': {
                'trajectory': trajectory,  # (1, obs_horizon, 2)
                'velocity': velocity,      # (1, 2)
                'position': position,      # (1, 2)
                'goal': goal               # (1, 2)
            },
            'pedestrians': {
                'trajectories': obs['pedestrian_observations'].unsqueeze(0),  # (1, max_peds, obs_horizon, 2)
                'mask': obs['pedestrian_mask'].unsqueeze(0)                   # (1, max_peds)
            },
            'corridors': {
                'polygons': obs['corridor_vertices'].unsqueeze(0),  # (1, max_corridors, max_vertices, 2)
                'vertex_counts': self._compute_vertex_counts(obs['corridor_vertices'], obs['corridor_mask']).unsqueeze(0),  # (1, max_corridors)
                'mask': obs['corridor_mask'].unsqueeze(0)          # (1, max_corridors)
            }
        }
        
        return model_obs
    
    def _compute_vertex_counts(self, corridor_vertices: torch.Tensor, 
                               corridor_mask: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—æ¯ä¸ªèµ°å»Šçš„å®é™…é¡¶ç‚¹æ•°
        
        Args:
            corridor_vertices: (max_corridors, max_vertices, 2)
            corridor_mask: (max_corridors,)
        
        Returns:
            vertex_counts: (max_corridors,)
        """
        max_corridors, max_vertices, _ = corridor_vertices.shape
        vertex_counts = torch.zeros(max_corridors, dtype=torch.long, device=corridor_vertices.device)
        
        for i in range(max_corridors):
            if corridor_mask[i]:
                # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå…¨é›¶çš„é¡¶ç‚¹ï¼ˆè¡¨ç¤ºpaddingå¼€å§‹ï¼‰
                vertices = corridor_vertices[i]  # (max_vertices, 2)
                norms = torch.norm(vertices, dim=1)  # (max_vertices,)
                non_zero = (norms > 1e-6).sum()
                vertex_counts[i] = max(non_zero.item(), 4)  # è‡³å°‘4ä¸ªé¡¶ç‚¹ï¼ˆçŸ©å½¢ï¼‰
            else:
                vertex_counts[i] = max_vertices  # paddingçš„èµ°å»Šä¹Ÿè®¾ç½®ä¸ºmax_vertices
        
        return vertex_counts
    
    def _copy_hidden_states(self, hidden_states: Dict) -> Dict:
        """æ·±æ‹·è´hidden states"""
        copied = {}
        for key, (h, c) in hidden_states.items():
            copied[key] = (h.clone(), c.clone())
        return copied
    
    def _to_device_observation(self, obs: Dict, device: str, deep_copy: bool = False) -> Dict:
        """
        å°†observationç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
        
        Args:
            obs: observation dict
            device: ç›®æ ‡è®¾å¤‡ ('cpu' or 'cuda')
            deep_copy: æ˜¯å¦æ·±æ‹·è´ï¼ˆé¿å…ä¿®æ”¹åŸå§‹æ•°æ®ï¼‰
        
        Returns:
            obs_on_device: ç§»åŠ¨åçš„observation
        """
        target_device = torch.device(device)
        
        def move_tensor(tensor):
            if deep_copy:
                return tensor.clone().detach().to(target_device)
            else:
                return tensor.to(target_device)
        
        def move_dict(d):
            result = {}
            for key, value in d.items():
                if isinstance(value, torch.Tensor):
                    result[key] = move_tensor(value)
                elif isinstance(value, dict):
                    result[key] = move_dict(value)
                else:
                    result[key] = value
            return result
        
        return move_dict(obs)
    
    def _compute_average_reward_components(self, episode_data: Dict) -> Optional[Dict]:
        """
        è®¡ç®—episodeä¸­å¥–åŠ±åˆ†é‡çš„å¹³å‡å€¼
        
        Args:
            episode_data: episodeæ•°æ®ï¼ˆåŒ…å«reward_infosï¼‰
        
        Returns:
            avg_rewards: å¹³å‡å¥–åŠ±åˆ†é‡å­—å…¸
        """
        if 'reward_infos' not in episode_data or not episode_data['reward_infos']:
            return None
        
        reward_infos = episode_data['reward_infos']
        
        # æ”¶é›†æ‰€æœ‰æ­¥éª¤çš„å¥–åŠ±åˆ†é‡
        progress_rewards = []
        direction_rewards = []
        curvature_rewards = []
        corridor_penalties = []
        corridor_violations = []
        goal_rewards = []
        collision_penalties = []
        step_penalties = []
        
        for info in reward_infos:
            if 'progress_reward' in info:
                progress_rewards.append(info['progress_reward'])
            if 'direction_reward' in info:
                direction_rewards.append(info['direction_reward'])
            if 'curvature_reward' in info:
                curvature_rewards.append(info['curvature_reward'])
            if 'corridor_penalty' in info:
                corridor_penalties.append(info['corridor_penalty'])
            if 'in_corridor' in info:
                corridor_violations.append(0 if info['in_corridor'] else 1)
            if 'goal_reached_reward' in info:
                goal_rewards.append(info['goal_reached_reward'])
            if 'collision_penalty' in info:
                collision_penalties.append(info['collision_penalty'])
            if 'step_penalty' in info:
                step_penalties.append(info['step_penalty'])
        
        # è®¡ç®—å¹³å‡å€¼å’Œç»Ÿè®¡
        avg_rewards = {
            'progress': np.mean(progress_rewards) if progress_rewards else 0.0,
            'direction': np.mean(direction_rewards) if direction_rewards else 0.0,
            'curvature': np.mean(curvature_rewards) if curvature_rewards else 0.0,
            'corridor': np.mean(corridor_penalties) if corridor_penalties else 0.0,
            'goal': np.sum(goal_rewards) if goal_rewards else 0.0,  # goalæ˜¯ç¨€ç–çš„ï¼Œç”¨sum
            'collision': np.sum(collision_penalties) if collision_penalties else 0.0,  # collisionæ˜¯ç¨€ç–çš„ï¼Œç”¨sum
            'step': np.mean(step_penalties) if step_penalties else 0.0,
            'corridor_violations': np.sum(corridor_violations) if corridor_violations else 0,  # è¿è§„æ¬¡æ•°
            'corridor_violation_rate': np.mean(corridor_violations) if corridor_violations else 0.0  # è¿è§„ç‡
        }
        
        return avg_rewards
    
    def _log_episode(self, episode: int, episode_return: float, 
                     episode_length: int, train_losses: Dict, 
                     episode_time: float, episode_data: Dict = None):
        """è®°å½•episodeæ—¥å¿—"""
        # æå–è·¯å¾„ä¿¡æ¯
        if episode_data is not None:
            start_pos = episode_data['start_pos']
            goal_pos = episode_data['goal_pos']
            actual_path = episode_data['actual_path']
            done_reason = episode_data['done_reason']
            
            # è®¡ç®—å®é™…è·ç¦»
            path_distance = 0.0
            for i in range(1, len(actual_path)):
                path_distance += np.linalg.norm(actual_path[i] - actual_path[i-1])
            
            # è®¡ç®—ç›´çº¿è·ç¦»å’Œæœ€ç»ˆè·ç¦»
            straight_distance = np.linalg.norm(goal_pos - start_pos)
            final_distance = np.linalg.norm(goal_pos - actual_path[-1])
            
            # ç¬¬ä¸€è¡Œï¼šåŸºç¡€ä¿¡æ¯
            log_str = f"[Episode {episode:4d}]"
            log_str += f" Return={episode_return:7.2f}"
            log_str += f" Length={episode_length:3d}"
            log_str += f" Buffer={len(self.buffer):4d}"
            
            # è®­ç»ƒæŸå¤±
            if train_losses:
                log_str += f" | Actor={train_losses['actor_loss']:.4f}"
                log_str += f" Critic={train_losses['critic_loss']:.4f}"
                log_str += f" Alpha={train_losses['alpha']:.4f}"
            
            # æ—¶é—´
            log_str += f" | Time={episode_time:.2f}s"
            print(log_str)
            

            # ç¬¬äºŒè¡Œï¼šè·¯å¾„è¯¦æƒ…ï¼ˆæ–°å¢ç¢°æ’ç±»å‹ç»Ÿè®¡ï¼‰
            path_str = f"  â”œâ”€ Start: ({start_pos[0]:5.2f},{start_pos[1]:5.2f})"
            path_str += f" â†’ Goal: ({goal_pos[0]:5.2f},{goal_pos[1]:5.2f})"
            path_str += f" | Dist: {path_distance:5.2f}m (ç›´çº¿:{straight_distance:.2f}m)"
            path_str += f" | å‰©ä½™: {final_distance:4.2f}m"
            path_str += f" | {done_reason}"
            
            # å¦‚æœæ˜¯ç¢°æ’ç»“æŸï¼Œæ˜¾ç¤ºç¢°æ’ç±»å‹
            if done_reason == 'collision' and 'reward_infos' in episode_data:
                collision_types = [info.get('collision_type', 'none') for info in episode_data['reward_infos']]
                # æ‰¾åˆ°æœ€åä¸€ä¸ªé'none'çš„ç¢°æ’ç±»å‹
                final_collision_type = 'none'
                for ct in reversed(collision_types):
                    if ct != 'none':
                        final_collision_type = ct
                        break
                
                # æ˜ å°„ç¢°æ’ç±»å‹ä¸ºä¸­æ–‡
                collision_type_map = {
                    'pedestrian': 'è¡Œäººç¢°æ’',
                    'corridor': 'corridorç¢°æ’',
                    'boundary': 'è¾¹ç•Œç¢°æ’',
                    'none': 'æœªçŸ¥'
                }
                path_str += f" [{collision_type_map.get(final_collision_type, final_collision_type)}]"
            
            print(path_str)
            
            # ç¬¬ä¸‰è¡Œï¼šå¥–åŠ±åˆ†é‡è¯¦æƒ…ï¼ˆæ–°å¢ï¼‰
            if 'reward_infos' in episode_data:
                # è®¡ç®—å¹³å‡å¥–åŠ±åˆ†é‡ï¼ˆå¦‚æœæœ‰å¤šä¸ªstepçš„è¯ï¼‰
                avg_rewards = self._compute_average_reward_components(episode_data)
                
                if avg_rewards:
                    reward_str = f"  â”œâ”€ Rewards: "
                    reward_str += f"Prog={avg_rewards['progress']:.3f} "
                    reward_str += f"Dir={avg_rewards['direction']:.3f} "
                    reward_str += f"Curv={avg_rewards['curvature']:.3f} "
                    reward_str += f"Corr={avg_rewards['corridor']:.3f} "
                    reward_str += f"Goal={avg_rewards['goal']:.1f} "
                    reward_str += f"Coll={avg_rewards['collision']:.1f} "
                    reward_str += f"Step={avg_rewards['step']:.3f}"
                    print(reward_str)
                    
                    # ç¬¬å››è¡Œï¼šCorridor violationç»Ÿè®¡
                    if avg_rewards['corridor_violations'] > 0:
                        corridor_str = f"  â”œâ”€ Corridor: "
                        corridor_str += f"Violations={int(avg_rewards['corridor_violations'])}/{episode_length} "
                        corridor_str += f"({avg_rewards['corridor_violation_rate']:.1%})"
                        print(corridor_str)
            
            # ç¬¬äº”è¡Œï¼šè·¯å¾„ç‚¹ï¼ˆæ¯5æ­¥æ˜¾ç¤ºä¸€ä¸ªï¼Œé¿å…å¤ªé•¿ï¼‰
            if len(actual_path) > 2:
                step_interval = max(1, len(actual_path) // 10)  # æœ€å¤šæ˜¾ç¤º10ä¸ªç‚¹
                sample_indices = list(range(0, len(actual_path), step_interval))
                if sample_indices[-1] != len(actual_path) - 1:
                    sample_indices.append(len(actual_path) - 1)  # ç¡®ä¿åŒ…å«æœ€åä¸€ä¸ªç‚¹
                
                path_points_str = f"  â””â”€ Path: "
                for idx in sample_indices[:10]:  # æœ€å¤šæ˜¾ç¤º10ä¸ªç‚¹
                    pos = actual_path[idx]
                    path_points_str += f"({pos[0]:5.2f},{pos[1]:5.2f})"
                    if idx != sample_indices[-1] or idx != len(actual_path) - 1:
                        path_points_str += " â†’ "
                
                if len(sample_indices) > 10:
                    path_points_str += "..."
                print(path_points_str)
            
            # æ”¶é›†è¯¦ç»†æ•°æ®åˆ°train_historyï¼ˆæ–°å¢ï¼‰
            self.train_history['done_reasons'].append(done_reason)
            
            # æå–ç¢°æ’ç±»å‹
            final_collision_type = 'none'
            if done_reason == 'collision' and 'reward_infos' in episode_data:
                collision_types = [info.get('collision_type', 'none') for info in episode_data['reward_infos']]
                for ct in reversed(collision_types):
                    if ct != 'none':
                        final_collision_type = ct
                        break
            self.train_history['collision_types'].append(final_collision_type)
            
            # æå–corridorè¿è§„å’Œå¥–åŠ±åˆ†é‡
            if 'reward_infos' in episode_data:
                avg_rewards = self._compute_average_reward_components(episode_data)
                if avg_rewards:
                    self.train_history['corridor_violations'].append(int(avg_rewards['corridor_violations']))
                    self.train_history['avg_progress_reward'].append(avg_rewards['progress'])
                    self.train_history['avg_corridor_penalty'].append(avg_rewards['corridor'])
                else:
                    # æ²¡æœ‰reward_infosæ—¶å¡«å……é»˜è®¤å€¼
                    self.train_history['corridor_violations'].append(0)
                    self.train_history['avg_progress_reward'].append(0.0)
                    self.train_history['avg_corridor_penalty'].append(0.0)
            else:
                # æ²¡æœ‰reward_infosæ—¶å¡«å……é»˜è®¤å€¼
                self.train_history['corridor_violations'].append(0)
                self.train_history['avg_progress_reward'].append(0.0)
                self.train_history['avg_corridor_penalty'].append(0.0)
                
        else:
            # æ—§ç‰ˆæœ¬å…¼å®¹ï¼šåªæ˜¾ç¤ºåŸºç¡€ä¿¡æ¯
            log_str = f"[Episode {episode:4d}]"
            log_str += f" Return={episode_return:7.2f}"
            log_str += f" Length={episode_length:3d}"
            log_str += f" Buffer={len(self.buffer):4d}"
            
            if train_losses:
                log_str += f" | Actor={train_losses['actor_loss']:.4f}"
                log_str += f" Critic={train_losses['critic_loss']:.4f}"
                log_str += f" Alpha={train_losses['alpha']:.4f}"
            
            log_str += f" | Time={episode_time:.2f}s"
            print(log_str)
            
            # æ—§ç‰ˆæœ¬å…¼å®¹ï¼šå¡«å……é»˜è®¤å€¼
            self.train_history['done_reasons'].append('unknown')
            self.train_history['collision_types'].append('none')
            self.train_history['corridor_violations'].append(0)
            self.train_history['avg_progress_reward'].append(0.0)
            self.train_history['avg_corridor_penalty'].append(0.0)
        
        # TensorBoardè®°å½•
        if self.use_tensorboard and self.writer is not None:
            self.writer.add_scalar('train/episode_return', episode_return, episode)
            self.writer.add_scalar('train/episode_length', episode_length, episode)
            self.writer.add_scalar('train/buffer_size', len(self.buffer), episode)
            
            if train_losses:
                self.writer.add_scalar('train/actor_loss', train_losses['actor_loss'], episode)
                self.writer.add_scalar('train/critic_loss', train_losses['critic_loss'], episode)
                self.writer.add_scalar('train/alpha', train_losses['alpha'], episode)
            
            self.writer.add_scalar('train/episode_time', episode_time, episode)
            
            # å¥–åŠ±åˆ†é‡è®°å½•ï¼ˆæ–°å¢ï¼‰
            if episode_data is not None and 'reward_infos' in episode_data:
                avg_rewards = self._compute_average_reward_components(episode_data)
                if avg_rewards:
                    self.writer.add_scalar('reward/progress', avg_rewards['progress'], episode)
                    self.writer.add_scalar('reward/direction', avg_rewards['direction'], episode)
                    self.writer.add_scalar('reward/curvature', avg_rewards['curvature'], episode)
                    self.writer.add_scalar('reward/corridor', avg_rewards['corridor'], episode)
                    self.writer.add_scalar('reward/goal', avg_rewards['goal'], episode)
                    self.writer.add_scalar('reward/collision', avg_rewards['collision'], episode)
                    self.writer.add_scalar('reward/step', avg_rewards['step'], episode)
                    
                    # Corridor violationç»Ÿè®¡
                    self.writer.add_scalar('corridor/violations', avg_rewards['corridor_violations'], episode)
                    self.writer.add_scalar('corridor/violation_rate', avg_rewards['corridor_violation_rate'], episode)
    
    def _log_evaluation(self, episode: int, eval_stats: Dict):
        """è®°å½•è¯„ä¼°æ—¥å¿—"""
        print(f"\n{'='*60}")
        print(f"[Evaluation @ Episode {episode}]")
        print(f"  Mean Return: {eval_stats['eval_return_mean']:.2f} Â± {eval_stats['eval_return_std']:.2f}")
        print(f"  Mean Length: {eval_stats['eval_length_mean']:.1f}")
        print(f"{'='*60}\n")
        
        # TensorBoardè®°å½•
        if self.use_tensorboard and self.writer is not None:
            self.writer.add_scalar('eval/mean_return', eval_stats['eval_return_mean'], episode)
            self.writer.add_scalar('eval/std_return', eval_stats['eval_return_std'], episode)
            self.writer.add_scalar('eval/mean_length', eval_stats['eval_length_mean'], episode)
    
    def save_checkpoint(self, is_best: bool = False, suffix: str = ''):
        """
        ä¿å­˜checkpoint
        
        Args:
            is_best: æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
            suffix: æ–‡ä»¶ååç¼€
        """
        # ç¡®å®šæ–‡ä»¶å
        if is_best:
            filename = 'best_model.pt'
        elif suffix:
            filename = f'checkpoint_{suffix}.pt'
        else:
            filename = f'checkpoint_ep{self.episode_count}.pt'
        
        filepath = self.save_dir / filename
        
        # ä¿å­˜å†…å®¹
        checkpoint = {
            'episode': self.episode_count,
            'total_steps': self.total_steps,
            'total_updates': self.total_updates,
            'model_state_dict': self.model.state_dict(),
            'best_eval_return': self.best_eval_return,
            'train_history': self.train_history
        }
        
        torch.save(checkpoint, filepath)
        
        if not is_best:
            print(f"  [Save] Checkpointä¿å­˜è‡³: {filename}")
    
    def load_checkpoint(self, filepath: str):
        """
        åŠ è½½checkpoint
        
        Args:
            filepath: checkpointæ–‡ä»¶è·¯å¾„
        """
        checkpoint = torch.load(filepath, map_location=self.device, weights_only=False)
        
        # æ¢å¤çŠ¶æ€
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.episode_count = checkpoint['episode']
        self.total_steps = checkpoint['total_steps']
        self.total_updates = checkpoint['total_updates']
        self.best_eval_return = checkpoint['best_eval_return']
        self.train_history = checkpoint['train_history']
        
        # å…¼å®¹æ€§ä¿®å¤ï¼šä¸ºæ—§checkpointæ·»åŠ æ–°å­—æ®µï¼ˆå¦‚æœç¼ºå¤±ï¼‰
        if 'done_reasons' not in self.train_history:
            print("[Load] æ£€æµ‹åˆ°æ—§ç‰ˆcheckpointï¼Œæ·»åŠ æ–°å­—æ®µ...")
            self.train_history['done_reasons'] = []
            self.train_history['collision_types'] = []
            self.train_history['corridor_violations'] = []
            self.train_history['avg_progress_reward'] = []
            self.train_history['avg_corridor_penalty'] = []
        
        # åŒæ­¥episode_countåˆ°ç¯å¢ƒï¼ˆç¡®ä¿resumeè®­ç»ƒæ—¶è¯¾ç¨‹å­¦ä¹ æ­£ç¡®ï¼‰
        if hasattr(self.env, 'episode_count'):
            self.env.episode_count = self.episode_count
            print(f"[Load] åŒæ­¥episode_countåˆ°ç¯å¢ƒ: {self.episode_count}")
        
        print(f"[Load] CheckpointåŠ è½½æˆåŠŸ: {filepath}")
        print(f"  - Episode: {self.episode_count}")
        print(f"  - Total steps: {self.total_steps}")
        print(f"  - Best eval return: {self.best_eval_return:.2f}")
    
    def save_training_history(self):
        """ä¿å­˜è®­ç»ƒå†å²ä¸ºJSON"""
        filepath = self.save_dir / 'training_history.json'
        
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
        history_serializable = {}
        for key, values in self.train_history.items():
            # åŒºåˆ†æ•°å€¼ç±»å‹å’Œå­—ç¬¦ä¸²ç±»å‹
            if key in ['done_reasons', 'collision_types']:
                # å­—ç¬¦ä¸²ç±»å‹ï¼Œç›´æ¥ä¿å­˜
                history_serializable[key] = values
            elif key == 'corridor_violations':
                # æ•´æ•°ç±»å‹
                history_serializable[key] = [int(v) for v in values]
            else:
                # æµ®ç‚¹æ•°ç±»å‹
                history_serializable[key] = [float(v) for v in values]
        
        with open(filepath, 'w') as f:
            json.dump(history_serializable, f, indent=2)
        
        print(f"[Save] è®­ç»ƒå†å²ä¿å­˜è‡³: training_history.json")
    
    def get_stats(self) -> Dict:
        """
        è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            stats: ç»Ÿè®¡å­—å…¸
        """
        stats = {
            'episode_count': self.episode_count,
            'total_steps': self.total_steps,
            'total_updates': self.total_updates,
            'buffer_size': len(self.buffer),
            'best_eval_return': self.best_eval_return
        }
        
        # æœ€è¿‘100ä¸ªepisodeçš„ç»Ÿè®¡
        if self.train_history['episode_returns']:
            recent_returns = self.train_history['episode_returns'][-100:]
            stats['recent_return_mean'] = np.mean(recent_returns)
            stats['recent_return_std'] = np.std(recent_returns)
        
        return stats


# ==================== å†…ç½®æµ‹è¯• ====================
if __name__ == '__main__':
    print("æµ‹è¯•AGSACTrainer...")
    
    # åˆ›å»ºdummyç¯å¢ƒå’Œæ¨¡å‹
    from ..envs import DummyAGSACEnvironment
    from ..models import AGSACModel
    
    print("\n1. åˆ›å»ºç¯å¢ƒå’Œæ¨¡å‹...")
    env = DummyAGSACEnvironment(
        max_pedestrians=5,   # å‡å°‘æ•°é‡
        max_corridors=5,     # å‡å°‘æ•°é‡
        max_vertices=10,     # å‡å°‘æ•°é‡
        obs_horizon=8,
        pred_horizon=12,
        max_episode_steps=50,
        device='cpu'
    )
    
    model = AGSACModel(
        dog_feature_dim=64,
        corridor_feature_dim=128,
        social_feature_dim=128,
        pedestrian_feature_dim=64,
        fusion_dim=64,
        action_dim=22,
        max_pedestrians=5,   # åŒ¹é…ç¯å¢ƒ
        max_corridors=5,     # åŒ¹é…ç¯å¢ƒ
        max_vertices=10,     # åŒ¹é…ç¯å¢ƒ
        device='cpu'
    )
    
    print("[OK] ç¯å¢ƒå’Œæ¨¡å‹åˆ›å»ºæˆåŠŸ")
    
    # 2. åˆ›å»ºTrainer
    print("\n2. åˆ›å»ºTrainer...")
    trainer = AGSACTrainer(
        model=model,
        env=env,
        buffer_capacity=100,
        seq_len=16,
        batch_size=4,
        warmup_episodes=2,
        updates_per_episode=5,
        eval_interval=5,
        eval_episodes=2,
        save_interval=10,
        log_interval=1,
        max_episodes=10,  # å°‘é‡episodesç”¨äºæµ‹è¯•
        device='cpu',
        save_dir='./test_outputs',
        experiment_name='test_agsac'
    )
    
    print("[OK] Traineråˆ›å»ºæˆåŠŸ")
    
    # 3. æµ‹è¯•collect_episode
    print("\n3. æµ‹è¯•collect_episode...")
    episode_data = trainer.collect_episode(deterministic=False)
    
    print(f"[OK] Episodeæ”¶é›†æˆåŠŸ")
    print(f"  - Episode length: {episode_data['episode_length']}")
    print(f"  - Episode return: {episode_data['episode_return']:.2f}")
    print(f"  - Fused states: {len(episode_data['fused_states'])}")
    print(f"  - Actions: {len(episode_data['actions'])}")
    
    # 4. æµ‹è¯•æ·»åŠ åˆ°buffer
    print("\n4. æµ‹è¯•ReplayBuffer...")
    trainer.buffer.add_episode(episode_data)
    print(f"[OK] Episodeæ·»åŠ åˆ°buffer, å¤§å°: {len(trainer.buffer)}")
    
    # 5. æµ‹è¯•train_stepï¼ˆéœ€è¦è¶³å¤Ÿçš„æ•°æ®ï¼‰
    print("\n5. æµ‹è¯•train_step...")
    # æ”¶é›†æ›´å¤šepisodes
    for i in range(3):
        ep = trainer.collect_episode(deterministic=False)
        trainer.buffer.add_episode(ep)
    
    print(f"[OK] Bufferå¤§å°: {len(trainer.buffer)}")
    
    if len(trainer.buffer) >= trainer.warmup_episodes:
        losses = trainer.train_step()
        print(f"[OK] è®­ç»ƒæ›´æ–°æˆåŠŸ")
        print(f"  - Actor loss: {losses['actor_loss']:.4f}")
        print(f"  - Critic loss: {losses['critic_loss']:.4f}")
        print(f"  - Alpha: {losses['alpha']:.4f}")
    
    # 6. æµ‹è¯•evaluate
    print("\n6. æµ‹è¯•evaluate...")
    eval_stats = trainer.evaluate()
    print(f"[OK] è¯„ä¼°å®Œæˆ")
    print(f"  - Mean return: {eval_stats['eval_return_mean']:.2f}")
    print(f"  - Std return: {eval_stats['eval_return_std']:.2f}")
    
    # 7. æµ‹è¯•å®Œæ•´è®­ç»ƒå¾ªç¯ï¼ˆçŸ­ç‰ˆï¼‰
    print("\n7. æµ‹è¯•å®Œæ•´è®­ç»ƒå¾ªç¯...")
    history = trainer.train()
    
    print(f"[OK] è®­ç»ƒå®Œæˆ")
    print(f"  - Episodes: {trainer.episode_count}")
    print(f"  - Total steps: {trainer.total_steps}")
    
    # 8. æµ‹è¯•checkpoint
    print("\n8. æµ‹è¯•checkpoint...")
    trainer.save_checkpoint(is_best=False, suffix='test')
    
    # åˆ›å»ºæ–°trainerå¹¶åŠ è½½
    new_trainer = AGSACTrainer(
        model=AGSACModel(device='cpu'),
        env=env,
        device='cpu',
        save_dir='./test_outputs',
        experiment_name='test_agsac'
    )
    
    checkpoint_path = trainer.save_dir / 'checkpoint_test.pt'
    new_trainer.load_checkpoint(str(checkpoint_path))
    
    print(f"[OK] CheckpointåŠ è½½æˆåŠŸ")
    
    # 9. æµ‹è¯•get_stats
    print("\n9. æµ‹è¯•get_stats...")
    stats = trainer.get_stats()
    print(f"[OK] ç»Ÿè®¡ä¿¡æ¯:")
    for k, v in stats.items():
        if isinstance(v, float):
            print(f"  - {k}: {v:.2f}")
        else:
            print(f"  - {k}: {v}")
    
    print("\n" + "="*60)
    print("[SUCCESS] AGSACTraineræ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    print("="*60)
    
    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    import shutil
    test_dir = Path('./test_outputs')
    if test_dir.exists():
        shutil.rmtree(test_dir)
        print("\n[Clean] æµ‹è¯•æ–‡ä»¶å·²æ¸…ç†")

