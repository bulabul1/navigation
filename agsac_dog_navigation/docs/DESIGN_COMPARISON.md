# åŸæ–¹æ¡ˆ vs å½“å‰å®ç°å¯¹æ¯”

## ğŸ“Š å…³é”®å·®å¼‚æ€»ç»“

### 1. **è®­ç»ƒæ–¹å¼** â­ æ ¸å¿ƒå˜åŒ–

| ç»´åº¦ | åŸæ–¹æ¡ˆ | å½“å‰å®ç° |
|-----|--------|---------|
| é‡‡æ ·å•ä½ | å•ä¸ªtransition | åºåˆ—æ®µsegment (seq_len=16) |
| å­˜å‚¨æ ¼å¼ | `{'state', 'action', 'reward', 'next_state', 'done', 'hidden_states'}` | Episodeè½¨è¿¹ï¼Œé‡‡æ ·æ—¶æå–segment |
| Hiddenç®¡ç† | æ¯ä¸ªtransitionå­˜å‚¨h | Segmentèµ·å§‹å­˜å‚¨hï¼Œå†…éƒ¨æŒ‰åºå±•å¼€ |
| SACæ›´æ–° | å•æ­¥Q-learning | åºåˆ—æ®µå±•å¼€LSTMè¿›è¡ŒQ-learning |
| Burn-in | æ—  | æ”¯æŒburn-iné¢„çƒ­hidden state |

**ç†ç”±**: LSTMéœ€è¦æ—¶åºä¸Šä¸‹æ–‡æ‰èƒ½å……åˆ†åˆ©ç”¨è®°å¿†èƒ½åŠ›ã€‚å•æ­¥é‡‡æ ·ä¼šä¸¢å¤±åºåˆ—ä¿¡æ¯ã€‚

---

### 2. **ReplayBufferè®¾è®¡**

#### åŸæ–¹æ¡ˆï¼ˆå•æ­¥ï¼‰
```python
buffer.add({
    'state': fused_state,      # (64,)
    'action': action,           # (22,)
    'reward': reward,
    'next_state': next_state,
    'done': done,
    'hidden_states': {
        'h_actor': (h, c),
        'h_critic1': (h, c),
        'h_critic2': (h, c)
    }
})

batch = buffer.sample(batch_size=256)  # 256ä¸ªç‹¬ç«‹transition
```

#### å½“å‰å®ç°ï¼ˆåºåˆ—æ®µï¼‰
```python
# å­˜å‚¨å®Œæ•´episode
episode_data = {
    'observations': [...],      # Tä¸ªè§‚æµ‹
    'actions': [...],           # Tä¸ªåŠ¨ä½œ
    'rewards': [...],           # Tä¸ªå¥–åŠ±
    'dones': [...],            # Tä¸ªdoneæ ‡å¿—
    'hidden_states': [...]     # Tä¸ªéšè—çŠ¶æ€
}
buffer.add_episode(episode_data)

# é‡‡æ ·segment
segment_batch = buffer.sample(batch_size=32)  # 32ä¸ªsegment
# æ¯ä¸ªsegment:
# {
#     'states': (seq_len, 64),
#     'actions': (seq_len, 22),
#     'rewards': (seq_len,),
#     'next_states': (seq_len, 64),
#     'dones': (seq_len,),
#     'init_hidden': segmentèµ·å§‹çš„hidden state
# }
```

---

### 3. **SAC Agentæ›´æ–°æµç¨‹**

#### åŸæ–¹æ¡ˆï¼ˆå•æ­¥æ›´æ–°ï¼‰
```python
def update(self, batch):
    states = batch['states']          # (256, 64)
    actions = batch['actions']        # (256, 22)
    rewards = batch['rewards']        # (256,)
    next_states = batch['next_states'] # (256, 64)
    dones = batch['dones']            # (256,)
    
    # æ¯ä¸ªæ ·æœ¬ç‹¬ç«‹å¤„ç†ï¼Œä¸è€ƒè™‘æ—¶åº
    with torch.no_grad():
        next_actions, next_log_probs = actor(next_states)
        target_q1 = critic1_target(next_states, next_actions)
        target_q2 = critic2_target(next_states, next_actions)
        target_q = min(target_q1, target_q2) - alpha * next_log_probs
        target = rewards + gamma * (1 - dones) * target_q
    
    current_q1 = critic1(states, actions)
    current_q2 = critic2(states, actions)
    
    critic_loss = F.mse_loss(current_q1, target) + F.mse_loss(current_q2, target)
    # ...
```

#### å½“å‰å®ç°ï¼ˆåºåˆ—æ®µæ›´æ–°ï¼‰
```python
def update(self, segment_batch):
    """
    segment_batch: List[Dict]ï¼Œæ¯ä¸ªsegmentæ˜¯ä¸€ä¸ªåºåˆ—
    """
    for segment in segment_batch:
        states = segment['states']          # (seq_len, 64)
        actions = segment['actions']        # (seq_len, 22)
        rewards = segment['rewards']        # (seq_len,)
        next_states = segment['next_states'] # (seq_len, 64)
        dones = segment['dones']            # (seq_len,)
        init_hidden = segment['init_hidden'] # èµ·å§‹éšè—çŠ¶æ€
        
        # æŒ‰æ—¶åºå±•å¼€LSTM
        seq_len = states.shape[0]
        h_actor = init_hidden['actor']
        h_critic1 = init_hidden['critic1']
        h_critic2 = init_hidden['critic2']
        
        # é€æ­¥å±•å¼€ï¼ˆæˆ–ä½¿ç”¨LSTMçš„batch_firstæ¨¡å¼ï¼‰
        for t in range(seq_len):
            state_t = states[t]
            action_t = actions[t]
            reward_t = rewards[t]
            next_state_t = next_states[t]
            done_t = dones[t]
            
            # Actorå‰å‘ï¼ˆæ›´æ–°h_actorï¼‰
            next_action, next_log_prob, h_actor = actor(next_state_t, h_actor)
            
            # Target Qè®¡ç®—
            target_q1, _ = critic1_target(next_state_t, next_action, ...)
            target_q2, _ = critic2_target(next_state_t, next_action, ...)
            target_q = min(target_q1, target_q2) - alpha * next_log_prob
            target = reward_t + gamma * (1 - done_t) * target_q
            
            # Current Qè®¡ç®—ï¼ˆæ›´æ–°h_criticï¼‰
            current_q1, h_critic1 = critic1(state_t, action_t, h_critic1)
            current_q2, h_critic2 = critic2(state_t, action_t, h_critic2)
            
            # ç´¯ç§¯æŸå¤±
            critic_loss += F.mse_loss(current_q1, target)
            critic_loss += F.mse_loss(current_q2, target)
        
        # å¹³å‡æŸå¤±
        critic_loss /= seq_len
```

**å…³é”®åŒºåˆ«**:
- åŸæ–¹æ¡ˆ: 256ä¸ªç‹¬ç«‹æ ·æœ¬å¹¶è¡Œè®¡ç®—
- å½“å‰å®ç°: 32ä¸ªsegmentï¼Œæ¯ä¸ªsegmentå†…éƒ¨æŒ‰æ—¶åºå±•å¼€

---

### 4. **å‚æ•°é‡é—®é¢˜** âš ï¸

| æ¨¡å— | åŸæ–¹æ¡ˆé¢„æœŸ | å½“å‰å®ç° | å·®å¼‚ |
|-----|-----------|---------|-----|
| SocialCircle | 20K (å†»ç»“) | 90K (å†…ç½®äºPredictor) | +70K |
| E-V2-Net | 300K (å†»ç»“) | 1.96M (SimplifiedE_V2_Net) | +1.66M |
| **æ€»è®¡** | **1.73M** (å«å†»ç»“) | **3.03M** | **+1.3M** |
| **å¯è®­ç»ƒ** | **1.41M** | **3.03M** | **è¶…å‡ºé¢„ç®—** |

**åŸå› åˆ†æ**:
- åŸæ–¹æ¡ˆå‡è®¾ä½¿ç”¨é¢„è®­ç»ƒçš„è½»é‡çº§E-V2-Net (300K)
- å½“å‰å®ç°çš„SimpleE_V2_Netä¸ºæ¯ä¸ªæ¨¡æ€åˆ›å»ºç‹¬ç«‹çš„GRUè§£ç å™¨
  - 20ä¸ªæ¨¡æ€ Ã— æ¯ä¸ª~100K = 2Må‚æ•°

**è§£å†³æ–¹æ¡ˆ**:
1. ä½¿ç”¨çœŸå®çš„é¢„è®­ç»ƒE-V2-Netï¼ˆéœ€ä¸‹è½½å¼€æºæƒé‡ï¼‰
2. é‡æ„ä¸ºå…±äº«è§£ç å™¨ + æ¨¡æ€åµŒå…¥
3. å‡å°‘æ¨¡æ€æ•° (20 â†’ 10)

---

### 5. **DogStateEncoderéšè—çŠ¶æ€**

| ç»´åº¦ | åŸæ–¹æ¡ˆ | å½“å‰å®ç° |
|-----|--------|---------|
| Hiddenè¿”å› | æ˜ç¡®è¿”å› | ä¸è¿”å›ï¼ˆå†…éƒ¨ç®¡ç†ï¼‰ |
| æ¥å£ | `forward(...) -> (features, hidden)` | `forward(...) -> features` |
| SAC Hidden | Actor/Criticç‹¬ç«‹ç®¡ç† | åªæœ‰Actor/Criticæœ‰hidden |

**å½“å‰å®ç°åˆç†æ€§**: DogEncoderçš„GRUæ˜¯ç‰¹å¾æå–ç”¨ï¼Œä¸éœ€è¦è·¨æ—¶é—´æ­¥è®°å¿†ã€‚SACçš„LSTMæ‰æ˜¯çœŸæ­£çš„å†³ç­–è®°å¿†ã€‚

---

### 6. **Alpha Lossè®¡ç®—** (å·²ä¿®æ­£)

#### åŸæ–¹æ¡ˆï¼ˆå¯èƒ½æœ‰bugï¼‰
```python
alpha_loss = -(log_alpha * (log_prob + target_entropy).detach()).mean()
```

#### å½“å‰å®ç°ï¼ˆç”¨æˆ·åé¦ˆåä¿®æ­£ï¼‰
```python
# ç´¯ç§¯æ‰€æœ‰segmentçš„log_prob
total_log_prob_for_alpha = 0
for segment in segment_batch:
    for t in range(seq_len):
        action, log_prob, _ = actor(state_t, h)
        total_log_prob_for_alpha += (log_prob + target_entropy).detach()

# è®¡ç®—æ€»æ ·æœ¬æ•°
total_samples = sum(segment['states'].shape[0] for segment in segment_batch)

# æ­£ç¡®çš„å¹³å‡
avg_log_prob = total_log_prob_for_alpha / total_samples
alpha_loss = -(log_alpha * avg_log_prob)
```

**å…³é”®**: é™¤ä»¥æ€»æ ·æœ¬æ•°ï¼Œè€Œébatch_sizeã€‚

---

### 7. **æ¨¡å—æ¥å£å·®å¼‚**

#### åŸæ–¹æ¡ˆé¢„æœŸ
```python
# SocialCircleç‹¬ç«‹æ¨¡å—
social_feat = social_circle(target_traj, neighbor_trajs, angles)

# E-V2-Netç‹¬ç«‹æ¨¡å—
future_pred = e_v2_net(social_feat)
```

#### å½“å‰å®ç°
```python
# æ•´åˆä¸ºTrajectoryPredictor
future_pred = trajectory_predictor(
    target_trajectory=target_traj,
    neighbor_trajectories=neighbor_trajs,
    neighbor_angles=angles,
    neighbor_mask=mask
)
# å†…éƒ¨è°ƒç”¨: SocialCircle â†’ E-V2-Net
```

**ä¼˜åŠ¿**: æ¥å£æ›´æ¸…æ™°ï¼Œä¾¿äºåˆ‡æ¢é¢„è®­ç»ƒ/ç®€åŒ–å®ç°ã€‚

---

### 8. **è§‚æµ‹æ ¼å¼ç»Ÿä¸€**

#### åŸæ–¹æ¡ˆ
```python
inputs_raw = {
    'pedestrian_past_trajs': [Tensor(8,2), ...],
    'corridors': [Tensor(N_i,2), ...],
    'dog_past_traj': Tensor(8,2),
    ...
}
```

#### å½“å‰å®ç°
```python
observation = {
    'dog': {
        'trajectory': (batch, 8, 2),
        'velocity': (batch, 2),
        'position': (batch, 2),
        'goal': (batch, 2)
    },
    'pedestrians': {
        'trajectories': (batch, max_peds, 8, 2),
        'mask': (batch, max_peds)
    },
    'corridors': {
        'polygons': (batch, max_corridors, max_vertices, 2),
        'vertex_counts': (batch, max_corridors),
        'mask': (batch, max_corridors)
    },
    'reference_line': (batch, 2, 2)
}
```

**æ”¹è¿›**: 
- å·²ç»åŒ…å«batchç»´åº¦
- å·²ç»å®Œæˆpaddingå’Œmask
- ç»“æ„åŒ–æ›´æ¸…æ™°

---

## âœ… ä¿æŒä¸€è‡´çš„éƒ¨åˆ†

1. âœ… **ç½‘ç»œæ¶æ„**: DogEncoder, CorridorEncoder, PedestrianEncoder, Fusionç»“æ„åŸºæœ¬ä¸€è‡´
2. âœ… **ç‰¹å¾ç»´åº¦**: 
   - Dog: 64
   - Pedestrian: 64
   - Corridor: 128
   - Fusion: 64
3. âœ… **SACç»“æ„**: Actor/Criticéƒ½æ˜¯PreFC + LSTM + Head
4. âœ… **GDE**: å‡ ä½•å¾®åˆ†è¯„ä¼°å™¨å®Œå…¨ä¸€è‡´
5. âœ… **åŠ¨ä½œç©ºé—´**: (11, 2) = 22ç»´
6. âœ… **Paddingç­–ç•¥**: max_pedestrians=10, max_corridors=5
7. âœ… **Maskæœºåˆ¶**: æ‰€æœ‰ç¼–ç å™¨æ­£ç¡®å¤„ç†mask

---

## ğŸ¯ éœ€è¦è¡¥å……çš„åŠŸèƒ½

### å½“å‰ç¼ºå¤±ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰

1. **SequenceReplayBuffer** - åºåˆ—æ®µé‡‡æ ·
2. **AGSACEnvironment** - ç¯å¢ƒæ¥å£
3. **AGSACTrainer** - è®­ç»ƒå¾ªç¯
4. **å‚æ•°ä¼˜åŒ–** - å°†3Mé™è‡³2Mä»¥å†…
5. **é…ç½®ç³»ç»Ÿ** - YAMLè¶…å‚æ•°ç®¡ç†
6. **è®­ç»ƒè„šæœ¬** - train.py, evaluate.py

---

## ğŸ“ å»ºè®®çš„ä¸‹ä¸€æ­¥

### é€‰é¡¹A: å®Œå–„å½“å‰æ¶æ„ï¼ˆæ¨èï¼‰
1. ä¼˜åŒ–TrajectoryPredictorå‚æ•°é‡
2. å®ç°SequenceReplayBuffer
3. å®ç°Environmentå’ŒTrainer
4. ç«¯åˆ°ç«¯æµ‹è¯•

### é€‰é¡¹B: å›å½’åŸæ–¹æ¡ˆ
1. æ”¹ä¸ºå•æ­¥é‡‡æ ·
2. ç§»é™¤segmenté€»è¾‘
3. ç®€åŒ–hidden stateç®¡ç†

**æ¨èé€‰é¡¹A**: åºåˆ—æ®µè®­ç»ƒæ›´é€‚åˆLSTMï¼Œå·²æŠ•å…¥çš„å·¥ä½œå¯ä»¥ä¿ç•™ã€‚

