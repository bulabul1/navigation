"""
SAC Agentä¸»æ§åˆ¶å™¨
æ•´åˆActorå’ŒCriticï¼Œå®ç°å®Œæ•´çš„SACè®­ç»ƒæµç¨‹
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from typing import Dict, List, Tuple, Optional
import copy

from .actor import HybridActor
from .critic import TwinCritic


class SACAgent(nn.Module):
    """
    Soft Actor-Critic Agent
    
    ç»„ä»¶ï¼š
    - actor: ç­–ç•¥ç½‘ç»œï¼ˆHybridActorï¼‰
    - critic: åŒQç½‘ç»œï¼ˆTwinCriticï¼‰
    - critic_target: ç›®æ ‡Qç½‘ç»œ
    - log_alpha: ç†µç³»æ•°ï¼ˆå¯å­¦ä¹ æˆ–å›ºå®šï¼‰
    
    åŠŸèƒ½ï¼š
    - select_action(): æ¨ç†/æ•°æ®æ”¶é›†
    - update(): è®­ç»ƒæ›´æ–°ï¼ˆæ”¯æŒåºåˆ—ç‰‡æ®µï¼‰
    - soft_update_target(): è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
    - save_checkpoint()/load_checkpoint(): æ¨¡å‹ä¿å­˜åŠ è½½
    """
    
    def __init__(
        self,
        state_dim: int = 64,
        action_dim: int = 22,
        hidden_dim: int = 128,
        actor_lr: float = 1e-4,
        critic_lr: float = 1e-4,
        alpha_lr: float = 3e-4,
        gamma: float = 0.99,
        tau: float = 0.005,
        auto_entropy: bool = True,
        target_entropy: Optional[float] = None,
        fixed_alpha: float = 0.2,
        max_grad_norm: float = 1.0,
        device: str = 'cpu'
    ):
        """
        Args:
            state_dim: çŠ¶æ€ç»´åº¦
            action_dim: åŠ¨ä½œç»´åº¦
            hidden_dim: LSTMéšè—å±‚ç»´åº¦
            actor_lr: Actorå­¦ä¹ ç‡
            critic_lr: Criticå­¦ä¹ ç‡
            alpha_lr: ç†µç³»æ•°å­¦ä¹ ç‡
            gamma: æŠ˜æ‰£å› å­
            tau: è½¯æ›´æ–°ç³»æ•°
            auto_entropy: æ˜¯å¦è‡ªåŠ¨è°ƒèŠ‚ç†µç³»æ•°
            target_entropy: ç›®æ ‡ç†µï¼ˆé»˜è®¤ä¸º-action_dimï¼‰
            max_grad_norm: æ¢¯åº¦è£å‰ªé˜ˆå€¼
            device: è®¾å¤‡
        """
        super().__init__()
        
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        self.gamma = gamma
        self.tau = tau
        self.auto_entropy = auto_entropy
        self.max_grad_norm = max_grad_norm
        self.device = torch.device(device)
        
        # æ€§èƒ½åˆ†ææ ‡å¿—
        self._profiled_update = False
        
        # ç›®æ ‡ç†µï¼ˆé»˜è®¤ä¸º-action_dimï¼‰
        if target_entropy is None:
            self.target_entropy = -action_dim
        else:
            self.target_entropy = target_entropy
        
        # åˆ›å»ºç½‘ç»œ
        self.actor = HybridActor(
            state_dim=state_dim,
            action_dim=action_dim,
            hidden_dim=hidden_dim
        ).to(self.device)
        
        self.critic = TwinCritic(
            state_dim=state_dim,
            action_dim=action_dim,
            hidden_dim=hidden_dim
        ).to(self.device)
        
        self.critic_target = TwinCritic(
            state_dim=state_dim,
            action_dim=action_dim,
            hidden_dim=hidden_dim
        ).to(self.device)
        
        # ç¡¬æ‹·è´criticå‚æ•°åˆ°critic_target
        self.critic_target.load_state_dict(self.critic.state_dict())
        
        # å†»ç»“targetç½‘ç»œï¼ˆä¸éœ€è¦æ¢¯åº¦ï¼‰
        for param in self.critic_target.parameters():
            param.requires_grad = False
        
        # ä¼˜åŒ–å™¨
        self.actor_optimizer = optim.Adam(
            self.actor.parameters(),
            lr=actor_lr
        )
        self.critic_optimizer = optim.Adam(
            self.critic.parameters(),
            lr=critic_lr
        )
        
        # ç†µç³»æ•°
        if auto_entropy:
            # å¯å­¦ä¹ çš„log(alpha)
            self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)
            self.alpha_optimizer = optim.Adam([self.log_alpha], lr=alpha_lr)
            self.alpha = self.log_alpha.exp()
        else:
            # å›ºå®šalpha
            self.log_alpha = torch.log(torch.tensor(float(fixed_alpha), device=self.device))
            self.alpha = self.log_alpha.exp()
            self.alpha_optimizer = None
        
        # è®­ç»ƒæ­¥æ•°
        self.total_updates = 0
    
    def select_action(
        self,
        state: torch.Tensor,
        hidden_actor: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        deterministic: bool = False
    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
        """
        é€‰æ‹©åŠ¨ä½œï¼ˆæ¨ç†/æ•°æ®æ”¶é›†ï¼‰
        
        Args:
            state: (64,) çŠ¶æ€
            hidden_actor: Actorçš„éšè—çŠ¶æ€
            deterministic: æ˜¯å¦ç¡®å®šæ€§ç­–ç•¥
        
        Returns:
            action: (22,) åŠ¨ä½œ
            new_hidden_actor: æ–°çš„éšè—çŠ¶æ€
        """
        with torch.no_grad():
            action, new_hidden_actor = self.actor.select_action(
                state, hidden_actor, deterministic
            )
        return action, new_hidden_actor
    
    def update(self, segment_batch: List[Dict]) -> Dict[str, float]:
        """
        ä½¿ç”¨åºåˆ—ç‰‡æ®µbatchæ›´æ–°ç½‘ç»œ
        
        Args:
            segment_batch: List[dict]ï¼Œæ¯ä¸ªdictåŒ…å«:
                - states: (seq_len, 64) çŠ¶æ€åºåˆ—
                - actions: (seq_len, 22) åŠ¨ä½œåºåˆ—
                - rewards: (seq_len,) å¥–åŠ±åºåˆ—
                - next_states: (seq_len, 64) ä¸‹ä¸€çŠ¶æ€åºåˆ—
                - dones: (seq_len,) ç»ˆæ­¢æ ‡å¿—åºåˆ—
                - init_hidden_actor: (h, c) Actoråˆå§‹éšè—çŠ¶æ€
                - init_hidden_critic1: (h, c) Critic1åˆå§‹éšè—çŠ¶æ€
                - init_hidden_critic2: (h, c) Critic2åˆå§‹éšè—çŠ¶æ€
        
        Returns:
            logs: dictï¼ŒåŒ…å«æŸå¤±å’ŒæŒ‡æ ‡ä¿¡æ¯
        """
        import time
        
        # æ€§èƒ½åˆ†æï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡æ›´æ–°æ—¶ï¼‰
        enable_profile = not self._profiled_update
        if enable_profile:
            self._profiled_update = True
            print("\n" + "="*70)
            print("ğŸ” æ€§èƒ½åˆ†æ: SACæ›´æ–°å†…éƒ¨ç»†èŠ‚")
            print("="*70)
            update_start = time.time()
        
        # è®¾ç½®è®­ç»ƒæ¨¡å¼ï¼ˆä¿®å¤ï¼šç¡®ä¿LSTMå¯ä»¥åšbackwardï¼‰
        self.actor.train()
        self.critic.train()
        
        batch_size = len(segment_batch)
        
        # ç´¯ç§¯æŸå¤±
        total_critic_loss = 0.0
        total_actor_loss = 0.0
        total_alpha_loss = 0.0
        
        # ç´¯ç§¯æŒ‡æ ‡
        total_q1_mean = 0.0
        total_q2_mean = 0.0
        total_log_prob_mean = 0.0
        
        # æ›´æ–°alphaï¼ˆå¦‚æœå¯ç”¨è‡ªåŠ¨ç†µè°ƒèŠ‚ï¼‰
        if self.auto_entropy:
            self.alpha = self.log_alpha.exp()
        
        # ==================== 1. è®¡ç®—Critic Lossï¼ˆä¸ç«‹å³backwardï¼‰====================
        if enable_profile:
            critic_forward_start = time.time()
        
        for segment in segment_batch:
            states = segment['states'].to(self.device)  # (seq_len, 64)
            actions = segment['actions'].to(self.device)  # (seq_len, 22)
            rewards = segment['rewards'].to(self.device)  # (seq_len,)
            next_states = segment['next_states'].to(self.device)  # (seq_len, 64)
            dones = segment['dones'].to(self.device)  # (seq_len,)
            
            seq_len = states.shape[0]
            
            # åˆå§‹åŒ–éšè—çŠ¶æ€ï¼ˆä»segmentä¸­æå–æˆ–è®¾ä¸ºNoneï¼‰
            # Criticæ›´æ–°æ—¶hidden=Noneï¼ˆæ ¹æ®è®¾è®¡å†³ç­–ï¼‰
            init_hidden = segment.get('init_hidden', None)
            if init_hidden is not None and isinstance(init_hidden, dict):
                h_actor = init_hidden.get('actor', None)
                h_c1 = init_hidden.get('critic1', None)
                h_c2 = init_hidden.get('critic2', None)
            else:
                h_c1 = None
                h_c2 = None
                h_actor = None
            
            # é€æ­¥è®¡ç®—target Qå’Œcurrent Q
            for t in range(seq_len):
                # è®¡ç®—target Qï¼ˆä½¿ç”¨targetç½‘ç»œå’Œtarget actorï¼‰
                with torch.no_grad():
                    # ä½¿ç”¨Actoré‡‡æ ·ä¸‹ä¸€ä¸ªåŠ¨ä½œ
                    next_action, next_log_prob, h_actor = self.actor(
                        next_states[t], h_actor, deterministic=False
                    )
                    
                    # ä½¿ç”¨Target Criticè¯„ä¼°
                    next_q1, next_q2, _, _ = self.critic_target(
                        next_states[t], next_action, None, None
                    )
                    
                    # å–è¾ƒå°çš„Qå€¼ï¼ˆDouble Q-learningï¼‰
                    min_next_q = torch.min(next_q1, next_q2)
                    
                    # è®¡ç®—target Qï¼ˆå¸¦ç†µæ­£åˆ™åŒ–ï¼‰
                    target_q = rewards[t] + self.gamma * (1 - dones[t]) * (
                        min_next_q - self.alpha * next_log_prob
                    )
                
                # è®¡ç®—current Q
                curr_q1, curr_q2, h_c1, h_c2 = self.critic(
                    states[t], actions[t], h_c1, h_c2
                )
                
                # ç¡®ä¿ç»´åº¦ä¸€è‡´ï¼ˆtarget_qå’Œcurr_qéƒ½åº”è¯¥æ˜¯æ ‡é‡ï¼‰
                if target_q.dim() > 0:
                    target_q = target_q.squeeze()
                if curr_q1.dim() > 0:
                    curr_q1 = curr_q1.squeeze()
                if curr_q2.dim() > 0:
                    curr_q2 = curr_q2.squeeze()
                
                # ç´¯ç§¯CriticæŸå¤±ï¼ˆMSEï¼‰
                total_critic_loss += F.mse_loss(curr_q1, target_q)
                total_critic_loss += F.mse_loss(curr_q2, target_q)
                
                # è®°å½•æŒ‡æ ‡
                total_q1_mean += curr_q1.item()
                total_q2_mean += curr_q2.item()
        
        # å¹³å‡CriticæŸå¤±
        critic_loss = total_critic_loss / batch_size
        
        if enable_profile:
            critic_forward_time = time.time() - critic_forward_start
        
        # ==================== 2. è®¡ç®—Actor Lossï¼ˆä¸ç«‹å³backwardï¼‰====================
        if enable_profile:
            actor_forward_start = time.time()
        
        # æš‚æ—¶å†»ç»“Criticå‚æ•°ï¼Œé˜²æ­¢ActoræŸå¤±æ›´æ–°Criticå‚æ•°ï¼ˆä½†ä¿ç•™å¯¹è¾“å…¥çš„æ¢¯åº¦ï¼‰
        critic_requires_grad_backup = [p.requires_grad for p in self.critic.parameters()]
        for p in self.critic.parameters():
            p.requires_grad_(False)

        for segment in segment_batch:
            states = segment['states'].to(self.device)
            seq_len = states.shape[0]
            
            # é‡ç½®éšè—çŠ¶æ€ï¼ˆä»segmentä¸­æå–ï¼‰
            init_hidden = segment.get('init_hidden', None)
            if init_hidden is not None and isinstance(init_hidden, dict):
                h_actor = init_hidden.get('actor', None)
                if h_actor is not None and isinstance(h_actor, tuple):
                    h_actor = (h_actor[0].to(self.device), h_actor[1].to(self.device))
            else:
                h_actor = None
            
            # é€æ­¥é‡æ–°é‡‡æ ·åŠ¨ä½œå¹¶è®¡ç®—loss
            for t in range(seq_len):
                # é‡æ–°é‡‡æ ·åŠ¨ä½œ
                new_action, log_prob, h_actor = self.actor(
                    states[t], h_actor, deterministic=False
                )
                
                # ä½¿ç”¨Critic1è¯„ä¼°ï¼ˆhidden=Noneï¼‰
                q_value, _ = self.critic.q1(states[t], new_action, None)
                
                # Actor lossï¼ˆæœ€å¤§åŒ–Q - Î±*entropyï¼‰
                # ç­‰ä»·äºæœ€å°åŒ– Î±*log_prob - Q
                total_actor_loss += (self.alpha * log_prob - q_value)
                
                # è®°å½•log_prob
                total_log_prob_mean += log_prob.item()
        
        # å¹³å‡ActoræŸå¤±
        actor_loss = total_actor_loss / batch_size
        
        # æ¢å¤Criticå‚æ•°çš„requires_gradè®¾ç½®
        for p, rg in zip(self.critic.parameters(), critic_requires_grad_backup):
            p.requires_grad_(rg)

        if enable_profile:
            actor_forward_time = time.time() - actor_forward_start
        
        # ==================== 3. ç»„åˆLosså¹¶ç»Ÿä¸€Backward ====================
        if enable_profile:
            backward_start = time.time()
        
        # æ¸…ç©ºæ‰€æœ‰æ¢¯åº¦
        self.critic_optimizer.zero_grad()
        self.actor_optimizer.zero_grad()
        
        # ç»„åˆlossï¼ˆç´¯ç§¯åä¸€æ¬¡backwardï¼‰
        combined_loss = critic_loss + actor_loss
        
        # ç»Ÿä¸€backwardï¼ˆæ¢¯åº¦ä¼ æ’­åˆ°Critic + Actor + ç¼–ç å™¨ï¼‰
        combined_loss.backward()
        
        # åˆ†åˆ«è£å‰ªå’Œæ›´æ–°
        critic_grad_norm = torch.nn.utils.clip_grad_norm_(
            self.critic.parameters(), self.max_grad_norm
        )
        actor_grad_norm = torch.nn.utils.clip_grad_norm_(
            self.actor.parameters(), self.max_grad_norm
        )
        
        self.critic_optimizer.step()
        self.actor_optimizer.step()
        
        if enable_profile:
            backward_time = time.time() - backward_start
            # åˆå¹¶ä¸ºä¸€ä¸ªbackwardæ—¶é—´
            critic_backward_time = backward_time
            actor_backward_time = 0.0  # å·²åŒ…å«åœ¨ä¸Šé¢
        
        # ==================== 4. æ›´æ–°Alphaï¼ˆè‡ªåŠ¨ç†µè°ƒèŠ‚ï¼‰====================
        if enable_profile:
            alpha_start = time.time()
        
        alpha_loss = torch.tensor(0.0)
        if self.auto_entropy and self.alpha_optimizer is not None:
            self.alpha_optimizer.zero_grad()
            
            # é‡æ–°è®¡ç®—log_probï¼ˆéœ€è¦detachï¼‰
            total_log_prob_for_alpha = 0.0
            for segment in segment_batch:
                states = segment['states'].to(self.device)
                seq_len = states.shape[0]
                
                # è·å–åˆå§‹éšè—çŠ¶æ€
                init_hidden = segment.get('init_hidden', None)
                if init_hidden is not None and isinstance(init_hidden, dict):
                    h_actor = init_hidden.get('actor', None)
                    if h_actor is not None and isinstance(h_actor, tuple):
                        h_actor = (h_actor[0].to(self.device), h_actor[1].to(self.device))
                else:
                    h_actor = None
                
                for t in range(seq_len):
                    _, log_prob, h_actor = self.actor(
                        states[t], h_actor, deterministic=False
                    )
                    total_log_prob_for_alpha += log_prob.detach().item()
            
            # Alpha lossï¼ˆæœ€å°åŒ– -log_alpha * (log_prob + target_entropy)ï¼‰
            # è®¡ç®—æ€»æ ·æœ¬æ•°ï¼ˆæ‰€æœ‰segmentçš„seq_lenä¹‹å’Œï¼‰
            total_samples = sum(segment['states'].shape[0] for segment in segment_batch)
            avg_log_prob = total_log_prob_for_alpha / total_samples
            alpha_loss = -(
                self.log_alpha * (avg_log_prob + self.target_entropy)
            )
            
            alpha_loss.backward()
            self.alpha_optimizer.step()
            
            # æ›´æ–°alpha
            self.alpha = self.log_alpha.exp()
            
            total_alpha_loss = alpha_loss.item()
        else:
            total_alpha_loss = 0.0
        
        if enable_profile:
            alpha_time = time.time() - alpha_start if self.auto_entropy else 0
        
        # ==================== 5. è½¯æ›´æ–°Targetç½‘ç»œ ====================
        if enable_profile:
            target_update_start = time.time()
        
        self.soft_update_target()
        
        if enable_profile:
            target_update_time = time.time() - target_update_start
            total_update_time = time.time() - update_start
            
            # è¾“å‡ºè¯¦ç»†åˆ†æ
            print(f"\nBatchå¤§å°: {batch_size} segments")
            print(f"å¹³å‡Sequenceé•¿åº¦: {sum(s['states'].shape[0] for s in segment_batch) / batch_size:.1f}æ­¥")
            print("-"*70)
            print(f"  1. Critic Forward:     {critic_forward_time*1000:8.2f}ms ({critic_forward_time/total_update_time*100:5.1f}%)")
            print(f"  2. Actor Forward:      {actor_forward_time*1000:8.2f}ms ({actor_forward_time/total_update_time*100:5.1f}%)")
            print(f"  3. Combined Backward:  {critic_backward_time*1000:8.2f}ms ({critic_backward_time/total_update_time*100:5.1f}%)")
            print(f"     (Critic+Actoræ¢¯åº¦ä¸€æ¬¡ä¼ æ’­)")
            print(f"  4. Alphaæ›´æ–°:          {alpha_time*1000:8.2f}ms ({alpha_time/total_update_time*100:5.1f}%)")
            print(f"  5. Targetç½‘ç»œæ›´æ–°:    {target_update_time*1000:8.2f}ms ({target_update_time/total_update_time*100:5.1f}%)")
            print("-"*70)
            print(f"  æ€»è®¡:                  {total_update_time*1000:8.2f}ms")
            print("\nğŸ’¡ æ–¹æ¡ˆBç‰¹ç‚¹:")
            print("  - Criticå’ŒActor lossç»„åˆåç»Ÿä¸€backward")
            print("  - ç¼–ç å™¨æ¢¯åº¦ = Criticæ¢¯åº¦ + Actoræ¢¯åº¦ (multi-task)")
            print("  - ä¸éœ€è¦retain_graphï¼ŒèŠ‚çœæ˜¾å­˜")
            print("="*70 + "\n")
        
        # æ›´æ–°è®¡æ•°
        self.total_updates += 1
        
        # è¿”å›æ—¥å¿—
        logs = {
            'critic_loss': critic_loss.item(),
            'actor_loss': actor_loss.item(),
            'alpha_loss': total_alpha_loss,
            'q1_mean': total_q1_mean / (batch_size * seq_len),
            'q2_mean': total_q2_mean / (batch_size * seq_len),
            'log_prob_mean': total_log_prob_mean / (batch_size * seq_len),
            'alpha': self.alpha.item(),
            'critic_grad_norm': critic_grad_norm.item(),
            'actor_grad_norm': actor_grad_norm.item(),
            'total_updates': self.total_updates
        }
        
        return logs
    
    def soft_update_target(self):
        """è½¯æ›´æ–°targetç½‘ç»œ"""
        for target_param, param in zip(
            self.critic_target.parameters(),
            self.critic.parameters()
        ):
            target_param.data.copy_(
                self.tau * param.data + (1.0 - self.tau) * target_param.data
            )
    
    def save_checkpoint(self, filepath: str):
        """ä¿å­˜checkpoint"""
        checkpoint = {
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'critic_target_state_dict': self.critic_target.state_dict(),
            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),
            'log_alpha': self.log_alpha,
            'total_updates': self.total_updates,
        }
        
        if self.alpha_optimizer is not None:
            checkpoint['alpha_optimizer_state_dict'] = self.alpha_optimizer.state_dict()
        
        torch.save(checkpoint, filepath)
    
    def load_checkpoint(self, filepath: str):
        """åŠ è½½checkpoint"""
        checkpoint = torch.load(filepath, map_location=self.device)
        
        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])
        self.critic_target.load_state_dict(checkpoint['critic_target_state_dict'])
        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])
        
        self.log_alpha = checkpoint['log_alpha']
        self.alpha = self.log_alpha.exp()
        self.total_updates = checkpoint['total_updates']
        
        if self.alpha_optimizer is not None and 'alpha_optimizer_state_dict' in checkpoint:
            self.alpha_optimizer.load_state_dict(checkpoint['alpha_optimizer_state_dict'])
    
    def train(self, mode: bool = True):
        """è®¾ç½®è®­ç»ƒæ¨¡å¼"""
        super().train(mode)
        self.actor.train(mode)
        self.critic.train(mode)
        # targetç½‘ç»œå§‹ç»ˆä¿æŒevalæ¨¡å¼
        self.critic_target.eval()
        return self
    
    def eval(self):
        """è®¾ç½®è¯„ä¼°æ¨¡å¼"""
        return self.train(False)


if __name__ == '__main__':
    """ç®€å•æµ‹è¯•"""
    print("æµ‹è¯•SAC Agent...")
    
    # åˆ›å»ºAgent
    agent = SACAgent(
        state_dim=64,
        action_dim=22,
        hidden_dim=128,
        device='cpu'
    )
    
    print(f"âœ“ Agentåˆ›å»ºæˆåŠŸ")
    print(f"  - Actorå‚æ•°: {sum(p.numel() for p in agent.actor.parameters()):,}")
    print(f"  - Criticå‚æ•°: {sum(p.numel() for p in agent.critic.parameters()):,}")
    print(f"  - æ€»å‚æ•°: {sum(p.numel() for p in agent.parameters()):,}")
    
    # æµ‹è¯•select_action
    state = torch.randn(64)
    action, hidden = agent.select_action(state, deterministic=False)
    print(f"\nâœ“ select_actionæµ‹è¯•é€šè¿‡")
    print(f"  - Action shape: {action.shape}")
    print(f"  - Hidden shape: h{hidden[0].shape}, c{hidden[1].shape}")
    
    # æµ‹è¯•updateï¼ˆåˆ›å»ºç®€å•çš„segment batchï¼‰
    segment_batch = []
    for _ in range(2):  # batch_size=2
        segment = {
            'states': torch.randn(5, 64),  # seq_len=5
            'actions': torch.randn(5, 22),
            'rewards': torch.randn(5),
            'next_states': torch.randn(5, 64),
            'dones': torch.zeros(5),
            'init_hidden_actor': None,
            'init_hidden_critic1': None,
            'init_hidden_critic2': None,
        }
        segment_batch.append(segment)
    
    logs = agent.update(segment_batch)
    
    print(f"\nâœ“ updateæµ‹è¯•é€šè¿‡")
    print(f"  - Critic loss: {logs['critic_loss']:.4f}")
    print(f"  - Actor loss: {logs['actor_loss']:.4f}")
    print(f"  - Alpha: {logs['alpha']:.4f}")
    print(f"  - Q1 mean: {logs['q1_mean']:.4f}")
    print(f"  - Q2 mean: {logs['q2_mean']:.4f}")
    
    print("\nâœ… SAC AgentåŸºç¡€æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")

